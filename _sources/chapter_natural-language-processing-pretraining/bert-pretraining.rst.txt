
.. _sec_bert-pretraining:

Pretraining BERT
================


With the BERT model implemented in :numref:`sec_bert` and the
pretraining examples generated from the WikiText-2 dataset in
:numref:`sec_bert-dataset`, we will pretrain BERT on the WikiText-2
dataset in this section.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import mlx.core as mx
    import mlx.optimizers as optim
    from mlx import nn
    from d2l import mlx as d2l

To start, we load the WikiText-2 dataset as minibatches of pretraining
examples for masked language modeling and next sentence prediction. The
batch size is 512 and the maximum length of a BERT input sequence is 64.
Note that in the original BERT model, the maximum length is 512.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    batch_size, max_len = 512, 64
    train_iter, num_batches, vocab = d2l.load_data_wiki(batch_size, max_len)

Pretraining BERT
----------------

The original BERT has two versions of different model sizes
:cite:`Devlin.Chang.Lee.ea.2018`. The base model
(:math:`\textrm{BERT}_{\textrm{BASE}}`) uses 12 layers (Transformer
encoder blocks) with 768 hidden units (hidden size) and 12
self-attention heads. The large model
(:math:`\textrm{BERT}_{\textrm{LARGE}}`) uses 24 layers with 1024 hidden
units and 16 self-attention heads. Notably, the former has 110 million
parameters while the latter has 340 million parameters. For
demonstration with ease, we define a small BERT, using 2 layers, 128
hidden units, and 2 self-attention heads.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    net = d2l.BERTModel(len(vocab), num_hiddens=128, ffn_num_input=128,
                        ffn_num_hiddens=256, num_heads=2,
                        num_blks=2, dropout=0.2, hid_in_features=128,
                        mlm_in_features=128, nsp_in_features=128)
    
    loss = nn.losses.cross_entropy

Before defining the training loop, we define a helper function
``_get_batch_loss_bert``. Given the shard of training examples, this
function computes the loss for both the masked language modeling and
next sentence prediction tasks. Note that the final loss of BERT
pretraining is just the sum of both the masked language modeling loss
and the next sentence prediction loss.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    #@save
    def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,
                             segments_X, valid_lens_x,
                             pred_positions_X, mlm_weights_X,
                             mlm_Y, nsp_y):
        # Forward pass
        _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,
                                      valid_lens_x.reshape(-1),
                                      pred_positions_X)
        # Compute masked language model loss
        mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size),
                     mlm_Y.reshape(-1),
                     reduction='mean') *\
        mlm_weights_X.reshape(-1, 1)
        print(f"mlm_l.sum(): {mlm_l.sum()}")
        print(f"mlm_weights_X.sum(): {mlm_weights_X.sum()}")
        mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)
        # Compute next sentence prediction loss
        nsp_l = loss(nsp_Y_hat, nsp_y, reduction='mean')
        l = mlm_l + nsp_l
        return mlm_l, nsp_l, l

Invoking the two aforementioned helper functions, the following
``train_bert`` function defines the procedure to pretrain BERT (``net``)
on the WikiText-2 (``train_iter``) dataset. Training BERT can take very
long. Instead of specifying the number of epochs for training as in the
``train_ch13`` function (see :numref:`sec_image_augmentation`), the
input ``num_steps`` of the following function specifies the number of
iteration steps for training.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def train_bert(train_iter, net, loss, vocab_size, num_steps):
        trainer = optim.Adam(learning_rate=0.01)
        step, timer = 0, d2l.Timer()
        animator = d2l.Animator(xlabel='step', ylabel='loss',
                                xlim=[1, num_steps], legend=['mlm', 'nsp'])
        # Sum of masked language modeling losses, sum of next sentence prediction
        # losses, no. of sentence pairs, count
        metric = d2l.Accumulator(4)
        num_steps_reached = False
    
        def loss_fn(net, tokens_X, segments_X,
                         valid_lens_x, pred_positions_X,
                         mlm_weights_X, mlm_Y, nsp_y):
            mlm_l, nsp_l, l = _get_batch_loss_bert(
                net, loss, vocab_size, tokens_X, segments_X,
                valid_lens_x, pred_positions_X, mlm_weights_X,
                mlm_Y, nsp_y)
            return mlm_l, nsp_l, l
    
        while step < num_steps and not num_steps_reached:
            for samples in train_iter:
                tokens_X = mx.array(samples["token_ids"])
                segments_X = mx.array(samples["segments"])
                valid_lens_x = mx.array(samples["valid_lens"])
                pred_positions_X = mx.array(samples["pred_positions"])
                mlm_weights_X = mx.array(samples["mlm_weights"])
                mlm_Y = mx.array(samples["mlm_labels"])
                nsp_y = mx.array(samples["nsp_labels"])
    
                timer.start()
                value_and_grads_fn = nn.value_and_grad(net, loss_fn)
                (mlm_l, nsp_l, l), grads = value_and_grads_fn(
                    net, tokens_X, segments_X, valid_lens_x,
                    pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)
                trainer.update(net, grads)
    
    
                metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)
                timer.stop()
                animator.add(step + 1,
                             (metric[0] / metric[3], metric[1] / metric[3]))
                step += 1
                if step == num_steps:
                    num_steps_reached = True
                    break
    
        print(f'MLM loss {metric[0] / metric[3]:.3f}, '
              f'NSP loss {metric[1] / metric[3]:.3f}')
        print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec')

We can plot both the masked language modeling loss and the next sentence
prediction loss during BERT pretraining.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    train_bert(train_iter, net, loss, len(vocab), 50)


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    MLM loss 6.081, NSP loss 0.699
    5713.9 sentence pairs/sec



.. figure:: output_bert-pretraining_17f5f5_11_1.svg


Representing Text with BERT
---------------------------

After pretraining BERT, we can use it to represent single text, text
pairs, or any token in them. The following function returns the BERT
(``net``) representations for all tokens in ``tokens_a`` and
``tokens_b``.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def get_bert_encoding(net, tokens_a, tokens_b=None):
        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
        token_ids = mx.array(vocab[tokens])[None, :]
        segments = mx.array(segments)[None, :]
        valid_len = mx.expand_dims(mx.array(len(tokens)), axis=0)
        encoded_X, _, _ = net(token_ids, segments, valid_len)
        return encoded_X

Consider the sentence “a crane is flying”. Recall the input
representation of BERT as discussed in
:numref:`subsec_bert_input_rep`. After inserting special tokens
“<cls>” (used for classification) and “<sep>” (used for separation), the
BERT input sequence has a length of six. Since zero is the index of the
“<cls>” token, ``encoded_text[:, 0, :]`` is the BERT representation of
the entire input sentence. To evaluate the polysemy token “crane”, we
also print out the first three elements of the BERT representation of
the token.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    tokens_a = ['a', 'crane', 'is', 'flying']
    encoded_text = get_bert_encoding(net, tokens_a)
    # Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'
    encoded_text_cls = encoded_text[:, 0, :]
    encoded_text_crane = encoded_text[:, 2, :]
    encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    ((1, 6, 128), (1, 128), array([0.109139, -0.0253232, 0.867328], dtype=float32))



Now consider a sentence pair “a crane driver came” and “he just left”.
Similarly, ``encoded_pair[:, 0, :]`` is the encoded result of the entire
sentence pair from the pretrained BERT. Note that the first three
elements of the polysemy token “crane” are different from those when the
context is different. This supports that BERT representations are
context-sensitive.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']
    encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)
    # Tokens: '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just',
    # 'left', '<sep>'
    encoded_pair_cls = encoded_pair[:, 0, :]
    encoded_pair_crane = encoded_pair[:, 2, :]
    encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    ((1, 10, 128),
     (1, 128),
     array([0.100949, -0.0350644, 0.892001], dtype=float32))



In :numref:`chap_nlp_app`, we will fine-tune a pretrained BERT model
for downstream natural language processing applications.

Summary
-------

-  The original BERT has two versions, where the base model has 110
   million parameters and the large model has 340 million parameters.
-  After pretraining BERT, we can use it to represent single text, text
   pairs, or any token in them.
-  In the experiment, the same token has different BERT representation
   when their contexts are different. This supports that BERT
   representations are context-sensitive.

Exercises
---------

1. In the experiment, we can see that the masked language modeling loss
   is significantly higher than the next sentence prediction loss. Why?
2. Set the maximum length of a BERT input sequence to be 512 (same as
   the original BERT model). Use the configurations of the original BERT
   model such as :math:`\textrm{BERT}_{\textrm{LARGE}}`. Do you
   encounter any error when running this section? Why?
