
.. _sec_sh_async:

Asynchronous Successive Halving
===============================


As we have seen in :numref:`sec_rs_async`, we can accelerate HPO by
distributing the evaluation of hyperparameter configurations across
either multiple instances or multiples CPUs / GPUs on a single instance.
However, compared to random search, it is not straightforward to run
successive halving (SH) asynchronously in a distributed setting. Before
we can decide which configuration to run next, we first have to collect
all observations at the current rung level. This requires to synchronize
workers at each rung level. For example, for the lowest rung level
:math:`r_{\mathrm{min}}`, we first have to evaluate all
:math:`N = \eta^K` configurations, before we can promote the
:math:`\frac{1}{\eta}` of them to the next rung level.

In any distributed system, synchronization typically implies idle time
for workers. First, we often observe high variations in training time
across hyperparameter configurations. For example, assuming the number
of filters per layer is a hyperparameter, then networks with less
filters finish training faster than networks with more filters, which
implies idle worker time due to stragglers. Moreover, the number of
slots in a rung level is not always a multiple of the number of workers,
in which case some workers may even sit idle for a full batch.

Figure :numref:`synchronous_sh` shows the scheduling of synchronous SH
with :math:`\eta=2` for four different trials with two workers. We start
with evaluating Trial-0 and Trial-1 for one epoch and immediately
continue with the next two trials once they are finished. We first have
to wait until Trial-2 finishes, which takes substantially more time than
the other trials, before we can promote the best two trials, i.e.,
Trial-0 and Trial-3 to the next rung level. This causes idle time for
Worker-1. Then, we continue with Rung 1. Also, here Trial-3 takes longer
than Trial-0, which leads to an additional ideling time of Worker-0.
Once, we reach Rung-2, only the best trial, Trial-0, remains which
occupies only one worker. To avoid that Worker-1 idles during that time,
most implementaitons of SH continue already with the next round, and
start evaluating new trials (e.g Trial-4) on the first rung.

.. _synchronous_sh:

.. figure:: ../img/sync_sh.svg

   Synchronous successive halving with two workers.


Asynchronous successive halving (ASHA) :cite:`li-arxiv18` adapts SH to
the asynchronous parallel scenario. The main idea of ASHA is to promote
configurations to the next rung level as soon as we collected at least
:math:`\eta` observations on the current rung level. This decision rule
may lead to suboptimal promotions: configurations can be promoted to the
next rung level, which in hindsight do not compare favourably against
most others at the same rung level. On the other hand, we get rid of all
synchronization points this way. In practice, such suboptimal initial
promotions have only a modest impact on performance, not only because
the ranking of hyperparameter configurations is often fairly consistent
across rung levels, but also because rungs grow over time and reflect
the distribution of metric values at this level better and better. If a
worker is free, but no configuration can be promoted, we start a new
configuration with :math:`r = r_{\mathrm{min}}`, i.e the first rung
level.

:numref:`asha` shows the scheduling of the same configurations for
ASHA. Once Trial-1 finishes, we collect the results of two trials (i.e
Trial-0 and Trial-1) and immediately promote the better of them
(Trial-0) to the next rung level. After Trial-0 finishes on rung 1,
there are too few trials there in order to support a further promotion.
Hence, we continue with rung 0 and evaluate Trial-3. Once Trial-3
finishes, Trial-2 is still pending. At this point we have 3 trials
evaluated on rung 0 and one trial evaluated already on rung 1. Since
Trial-3 performs worse than Trial-0 at rung 0, and :math:`\eta=2`, we
cannot promote any new trial yet, and Worker-1 starts Trial-4 from
scratch instead. However, once Trial-2 finishes and scores worse than
Trial-3, the latter is promoted towards rung 1. Afterwards, we collected
2 evaluations on rung 1, which means we can now promote Trial-0 towards
rung 2. At the same time, Worker-1 continues with evaluating new trials
(i.e., Trial-5) on rung 0.

.. _asha:

.. figure:: ../img/asha.svg

   Asynchronous successive halving (ASHA) with two workers.


.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import logging
    from d2l import mlx as d2l
    
    logging.basicConfig(level=logging.INFO)
    import matplotlib.pyplot as plt
    from syne_tune import StoppingCriterion, Tuner
    from syne_tune.backend import PythonBackend
    from syne_tune.config_space import loguniform, randint
    from syne_tune.experiments import load_experiment
    from syne_tune.optimizer.baselines import ASHA

Objective Function
------------------

We will use *Syne Tune* with the same objective function as in
:numref:`sec_rs_async`.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def hpo_objective_lenet_synetune(learning_rate, batch_size, max_epochs):
        from syne_tune import Reporter
        from d2l import mlx as d2l
    
        model = d2l.LeNet(num_inputs=1,lr=learning_rate, num_classes=10)
        trainer = d2l.HPOTrainer(max_epochs=max_epochs, num_gpus=1)
        data = d2l.FashionMNIST(batch_size=batch_size)
        report = Reporter()
    
        for epoch in range(1, max_epochs + 1):
            if epoch == 1:
                # Initialize the state of Trainer
                trainer.fit(model=model, data=data)
            else:
                trainer.fit_epoch()
    
            validation_error = trainer.validation_error()
            report(epoch=epoch, validation_error=float(validation_error))

We will also use the same configuration space as before:

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    min_number_of_epochs = 2
    max_number_of_epochs = 10
    eta = 2
    
    config_space = {
        "learning_rate": loguniform(1e-2, 1),
        "batch_size": randint(32, 256),
        "max_epochs": max_number_of_epochs,
    }
    initial_config = {
        "learning_rate": 0.1,
        "batch_size": 128,
    }

Asynchronous Scheduler
----------------------

First, we define the number of workers that evaluate trials
concurrently. We also need to specify how long we want to run random
search, by defining an upper limit on the total wall-clock time.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    n_workers = 2  # Needs to be <= the number of available GPUs
    max_wallclock_time = 12 * 60  # 12 minutes

The code for running ASHA is a simple variation of what we did for
asynchronous random search.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    # mode = "min"
    # metric = "validation_error"
    # resource_attr = "epoch"
    
    scheduler = ASHA(
        config_space,
        metric="validation_error",
        time_attr="epoch",
        max_t=max_number_of_epochs,
        do_minimize=True,
        points_to_evaluate=[initial_config],
    )

Here, ``metric`` and ``resource_attr`` specify the key names used with
the ``report`` callback, and ``max_resource_attr`` denotes which input
to the objective function corresponds to :math:`r_{\mathrm{max}}`.
Moreover, ``grace_period`` provides :math:`r_{\mathrm{min}}`, and
``reduction_factor`` is :math:`\eta`. We can run Syne Tune as before
(this will take about 12 minutes):

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    trial_backend = PythonBackend(
        tune_function=hpo_objective_lenet_synetune,
        config_space=config_space,
    )
    
    stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
        print_update_interval=int(max_wallclock_time * 0.6),
    )
    tuner.run()


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    INFO:syne_tune.tuner:results of trials will be saved on /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190
    INFO:root:Error launching /usr/bin/nvidia-smi, no GPU could be detected.
    INFO:syne_tune.backend.local_backend:Detected 0 GPUs
    WARNING:syne_tune.backend.local_backend:num_gpus_per_trial = 1 is too large, reducing to 0
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.3890338391272654 --batch_size 248 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/0/checkpoints
    INFO:syne_tune.tuner:(trial 0) - scheduled config {'learning_rate': 0.3890338391272654, 'batch_size': 248, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.011335521846740686 --batch_size 162 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/1/checkpoints
    INFO:syne_tune.tuner:(trial 1) - scheduled config {'learning_rate': 0.011335521846740686, 'batch_size': 162, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.3472413436671253 --batch_size 213 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/2/checkpoints
    INFO:syne_tune.tuner:(trial 2) - scheduled config {'learning_rate': 0.3472413436671253, 'batch_size': 213, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 0 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.016820573610021133 --batch_size 115 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/3/checkpoints
    INFO:syne_tune.tuner:(trial 3) - scheduled config {'learning_rate': 0.016820573610021133, 'batch_size': 115, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.028005751626881802 --batch_size 64 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/4/checkpoints
    INFO:syne_tune.tuner:(trial 4) - scheduled config {'learning_rate': 0.028005751626881802, 'batch_size': 64, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.04981619915828577 --batch_size 34 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/5/checkpoints
    INFO:syne_tune.tuner:(trial 5) - scheduled config {'learning_rate': 0.04981619915828577, 'batch_size': 34, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.212063712661875 --batch_size 103 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/6/checkpoints
    INFO:syne_tune.tuner:(trial 6) - scheduled config {'learning_rate': 0.212063712661875, 'batch_size': 103, 'max_epochs': 10}
    start: fit model with 5 observations
    Time for fit model with 5 observations: 0.0001 secs
    INFO:syne_tune.tuner:Trial trial_id 6 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.38809559103096963 --batch_size 88 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/7/checkpoints
    INFO:syne_tune.tuner:(trial 7) - scheduled config {'learning_rate': 0.38809559103096963, 'batch_size': 88, 'max_epochs': 10}
    start: fit model with 7 observations
    Time for fit model with 7 observations: 0.0001 secs
    INFO:syne_tune.tuner:Trial trial_id 5 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.010988948291006459 --batch_size 235 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/8/checkpoints
    INFO:syne_tune.tuner:(trial 8) - scheduled config {'learning_rate': 0.010988948291006459, 'batch_size': 235, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.05600839449203463 --batch_size 166 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/9/checkpoints
    INFO:syne_tune.tuner:(trial 9) - scheduled config {'learning_rate': 0.05600839449203463, 'batch_size': 166, 'max_epochs': 10}
    start: fit model with 9 observations
    Time for fit model with 9 observations: 0.0001 secs
    INFO:syne_tune.tuner:Trial trial_id 7 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.01176482331903498 --batch_size 159 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/10/checkpoints
    INFO:syne_tune.tuner:(trial 10) - scheduled config {'learning_rate': 0.01176482331903498, 'batch_size': 159, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.016564846045842005 --batch_size 212 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/11/checkpoints
    INFO:syne_tune.tuner:(trial 11) - scheduled config {'learning_rate': 0.016564846045842005, 'batch_size': 212, 'max_epochs': 10}
    start: fit model with 10 observations
    Time for fit model with 10 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.28001134495568514 --batch_size 118 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/12/checkpoints
    INFO:syne_tune.tuner:(trial 12) - scheduled config {'learning_rate': 0.28001134495568514, 'batch_size': 118, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.3011893656031137 --batch_size 159 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/13/checkpoints
    INFO:syne_tune.tuner:(trial 13) - scheduled config {'learning_rate': 0.3011893656031137, 'batch_size': 159, 'max_epochs': 10}
    start: fit model with 12 observations
    Time for fit model with 12 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.08928561211360032 --batch_size 102 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/14/checkpoints
    INFO:syne_tune.tuner:(trial 14) - scheduled config {'learning_rate': 0.08928561211360032, 'batch_size': 102, 'max_epochs': 10}
    start: fit model with 14 observations
    Time for fit model with 14 observations: 0.0001 secs
    INFO:syne_tune.tuner:Trial trial_id 12 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.2583974707015322 --batch_size 216 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/15/checkpoints
    INFO:syne_tune.tuner:(trial 15) - scheduled config {'learning_rate': 0.2583974707015322, 'batch_size': 216, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.02994522036707356 --batch_size 248 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/16/checkpoints
    INFO:syne_tune.tuner:(trial 16) - scheduled config {'learning_rate': 0.02994522036707356, 'batch_size': 248, 'max_epochs': 10}
    start: fit model with 15 observations
    Time for fit model with 15 observations: 0.0001 secs
    INFO:syne_tune.tuner:tuning status (last metric is reported)
     trial_id     status  iter  learning_rate  batch_size  max_epochs  epoch  validation_error  worker-time
            0  Completed    10       0.389034         248          10   10.0          0.174823    46.390648
            1    Stopped     2       0.011336         162          10    2.0          0.899918    28.746251
            2    Stopped     4       0.347241         213          10    4.0          0.199671    33.409475
            3    Stopped     1       0.016821         115          10    1.0          0.810999    32.634836
            4    Stopped     1       0.028006          64          10    1.0          0.329817    44.935998
            5  Completed    10       0.049816          34          10   10.0          0.172483   128.607070
            6  Completed    10       0.212064         103          10   10.0          0.157916    80.433653
            7  Completed    10       0.388096          88          10   10.0          0.131109    72.157970
            8    Stopped     2       0.010989         235          10    2.0          0.900160    29.473000
            9    Stopped     1       0.056008         166          10    1.0          0.473938    29.698198
           10    Stopped     1       0.011765         159          10    1.0          0.900002    27.082558
           11    Stopped     2       0.016565         212          10    2.0          0.898847    27.910791
           12  Completed    10       0.280011         118          10   10.0          0.170069    63.088011
           13    Stopped     3       0.301189         159          10    3.0          0.216004    34.259483
           14    Stopped     1       0.089286         102          10    1.0          0.261735    33.068030
           15 InProgress     0       0.258397         216          10      -                 -            -
           16 InProgress     0       0.029945         248          10      -                 -            -
    2 trials running, 15 finished (5 until the end), 435.89s wallclock-time
    
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.014524661982273063 --batch_size 175 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/17/checkpoints
    INFO:syne_tune.tuner:(trial 17) - scheduled config {'learning_rate': 0.014524661982273063, 'batch_size': 175, 'max_epochs': 10}
    start: fit model with 16 observations
    Time for fit model with 16 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.09615916813911475 --batch_size 83 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/18/checkpoints
    INFO:syne_tune.tuner:(trial 18) - scheduled config {'learning_rate': 0.09615916813911475, 'batch_size': 83, 'max_epochs': 10}
    start: fit model with 17 observations
    Time for fit model with 17 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.06691128419125478 --batch_size 197 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/19/checkpoints
    INFO:syne_tune.tuner:(trial 19) - scheduled config {'learning_rate': 0.06691128419125478, 'batch_size': 197, 'max_epochs': 10}
    start: fit model with 18 observations
    Time for fit model with 18 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.20248813907299865 --batch_size 129 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/20/checkpoints
    INFO:syne_tune.tuner:(trial 20) - scheduled config {'learning_rate': 0.20248813907299865, 'batch_size': 129, 'max_epochs': 10}
    start: fit model with 19 observations
    Time for fit model with 19 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.011114022771092129 --batch_size 226 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/21/checkpoints
    INFO:syne_tune.tuner:(trial 21) - scheduled config {'learning_rate': 0.011114022771092129, 'batch_size': 226, 'max_epochs': 10}
    start: fit model with 20 observations
    Time for fit model with 20 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.1846580541163797 --batch_size 128 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/22/checkpoints
    INFO:syne_tune.tuner:(trial 22) - scheduled config {'learning_rate': 0.1846580541163797, 'batch_size': 128, 'max_epochs': 10}
    start: fit model with 21 observations
    Time for fit model with 21 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.08169016239095044 --batch_size 63 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/23/checkpoints
    INFO:syne_tune.tuner:(trial 23) - scheduled config {'learning_rate': 0.08169016239095044, 'batch_size': 63, 'max_epochs': 10}
    start: fit model with 22 observations
    Time for fit model with 22 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.04385133919593825 --batch_size 84 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/24/checkpoints
    INFO:syne_tune.tuner:(trial 24) - scheduled config {'learning_rate': 0.04385133919593825, 'batch_size': 84, 'max_epochs': 10}
    start: fit model with 23 observations
    Time for fit model with 23 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.038617876339801765 --batch_size 209 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/25/checkpoints
    INFO:syne_tune.tuner:(trial 25) - scheduled config {'learning_rate': 0.038617876339801765, 'batch_size': 209, 'max_epochs': 10}
    start: fit model with 24 observations
    Time for fit model with 24 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.41776684604758285 --batch_size 177 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/26/checkpoints
    INFO:syne_tune.tuner:(trial 26) - scheduled config {'learning_rate': 0.41776684604758285, 'batch_size': 177, 'max_epochs': 10}
    start: fit model with 25 observations
    Time for fit model with 25 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.016830288384432443 --batch_size 205 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/27/checkpoints
    INFO:syne_tune.tuner:(trial 27) - scheduled config {'learning_rate': 0.016830288384432443, 'batch_size': 205, 'max_epochs': 10}
    start: fit model with 26 observations
    Time for fit model with 26 observations: 0.0001 secs
    INFO:syne_tune.tuner:Trial trial_id 26 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.12345124308220282 --batch_size 101 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/28/checkpoints
    INFO:syne_tune.tuner:(trial 28) - scheduled config {'learning_rate': 0.12345124308220282, 'batch_size': 101, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.03716318260525985 --batch_size 96 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/29/checkpoints
    INFO:syne_tune.tuner:(trial 29) - scheduled config {'learning_rate': 0.03716318260525985, 'batch_size': 96, 'max_epochs': 10}
    start: fit model with 28 observations
    Time for fit model with 28 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.08300129677257653 --batch_size 97 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/30/checkpoints
    INFO:syne_tune.tuner:(trial 30) - scheduled config {'learning_rate': 0.08300129677257653, 'batch_size': 97, 'max_epochs': 10}
    start: fit model with 29 observations
    Time for fit model with 29 observations: 0.0001 secs
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.34819874766327785 --batch_size 249 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/tune_function --tune_function_hash 8d938717fd641d43907cdb9888ac1aaa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190/31/checkpoints
    INFO:syne_tune.tuner:(trial 31) - scheduled config {'learning_rate': 0.34819874766327785, 'batch_size': 249, 'max_epochs': 10}
    start: fit model with 30 observations
    Time for fit model with 30 observations: 0.0001 secs
    INFO:syne_tune.stopping_criterion:reaching max wallclock time (720), stopping there.
    INFO:syne_tune.tuner:Stopping trials that may still be running.
    INFO:syne_tune.tuner:Tuning finished, results of trials can be found on /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-06-26-16-190
    --------------------
    Resource summary (last result is reported):
     trial_id     status  iter  learning_rate  batch_size  max_epochs  epoch  validation_error  worker-time
            0  Completed    10       0.389034         248          10   10.0          0.174823    46.390648
            1    Stopped     2       0.011336         162          10    2.0          0.899918    28.746251
            2    Stopped     4       0.347241         213          10    4.0          0.199671    33.409475
            3    Stopped     1       0.016821         115          10    1.0          0.810999    32.634836
            4    Stopped     1       0.028006          64          10    1.0          0.329817    44.935998
            5  Completed    10       0.049816          34          10   10.0          0.172483   128.607070
            6  Completed    10       0.212064         103          10   10.0          0.157916    80.433653
            7  Completed    10       0.388096          88          10   10.0          0.131109    72.157970
            8    Stopped     2       0.010989         235          10    2.0          0.900160    29.473000
            9    Stopped     1       0.056008         166          10    1.0          0.473938    29.698198
           10    Stopped     1       0.011765         159          10    1.0          0.900002    27.082558
           11    Stopped     2       0.016565         212          10    2.0          0.898847    27.910791
           12  Completed    10       0.280011         118          10   10.0          0.170069    63.088011
           13    Stopped     3       0.301189         159          10    3.0          0.216004    34.259483
           14    Stopped     1       0.089286         102          10    1.0          0.261735    33.068030
           15    Stopped     2       0.258397         216          10    2.0          0.268789    26.619754
           16    Stopped     1       0.029945         248          10    1.0          0.899793    23.221629
           17    Stopped     1       0.014525         175          10    1.0          0.899113    29.563362
           18    Stopped     1       0.096159          83          10    1.0          0.234850    37.190157
           19    Stopped     1       0.066911         197          10    1.0          0.466594    27.127466
           20    Stopped     1       0.202488         129          10    1.0          0.237821    29.175115
           21    Stopped     1       0.011114         226          10    1.0          0.900478    26.078950
           22    Stopped     1       0.184658         128          10    1.0          0.238133    32.205698
           23    Stopped     1       0.081690          63          10    1.0          0.247822    43.514397
           24    Stopped     2       0.043851          84          10    2.0          0.278472    39.559851
           25    Stopped     2       0.038618         209          10    2.0          0.899923    28.638117
           26  Completed    10       0.417767         177          10   10.0          0.155831    52.305785
           27    Stopped     2       0.016830         205          10    2.0          0.899888    28.042010
           28    Stopped     1       0.123451         101          10    1.0          0.262178    34.001491
           29    Stopped     1       0.037163          96          10    1.0          0.414087    35.973114
           30 InProgress     0       0.083001          97          10      -                 -            -
           31 InProgress     0       0.348199         249          10      -                 -            -
    2 trials running, 30 finished (6 until the end), 721.60s wallclock-time
    
    validation_error: best 0.13110899872947157 for trial-id 7
    --------------------


Note that we are running a variant of ASHA where underperforming trials
are stopped early. This is different to our implementation in
:numref:`sec_mf_hpo_sh`, where each training job is started with a
fixed ``max_epochs``. In the latter case, a well-performing trial which
reaches the full 10 epochs, first needs to train 1, then 2, then 4, then
8 epochs, each time starting from scratch. This type of pause-and-resume
scheduling can be implemented efficiently by checkpointing the training
state after each epoch, but we avoid this extra complexity here. After
the experiment has finished, we can retrieve and plot results.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    d2l.set_figsize()
    e = load_experiment(tuner.name)
    e.plot()



.. figure:: output_sh-async_950d48_13_0.svg


Visualize the Optimization Process
----------------------------------

Once more, we visualize the learning curves of every trial (each color
in the plot represents a trial). Compare this to asynchronous random
search in :numref:`sec_rs_async`. As we have seen for successive
halving in :numref:`sec_mf_hpo`, most of the trials are stopped at 1
or 2 epochs (:math:`r_{\mathrm{min}}` or
:math:`\eta * r_{\mathrm{min}}`). However, trials do not stop at the
same point, because they require different amount of time per epoch. If
we ran standard successive halving instead of ASHA, we would need to
synchronize our workers, before we can promote configurations to the
next rung level.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    d2l.set_figsize([6, 2.5])
    results = e.results
    for trial_id in results.trial_id.unique():
        df = results[results["trial_id"] == trial_id]
        d2l.plt.plot(
            df["st_tuner_time"],
            df["validation_error"],
            marker="o"
        )
    d2l.plt.xlabel("wall-clock time")
    d2l.plt.ylabel("objective function")




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    Text(0, 0.5, 'objective function')




.. figure:: output_sh-async_950d48_15_1.svg


Summary
-------

Compared to random search, successive halving is not quite as trivial to
run in an asynchronous distributed setting. To avoid synchronisation
points, we promote configurations as quickly as possible to the next
rung level, even if this means promoting some wrong ones. In practice,
this usually does not hurt much, and the gains of asynchronous versus
synchronous scheduling are usually much higher than the loss of the
suboptimal decision making.
