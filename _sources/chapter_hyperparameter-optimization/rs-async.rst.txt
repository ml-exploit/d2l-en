
.. _sec_rs_async:

Asynchronous Random Search
==========================


As we have seen in the previous :numref:`sec_api_hpo`, we might have
to wait hours or even days before random search returns a good
hyperparameter configuration, because of the expensive evaluation of
hyperparameter configurations. In practice, we have often access to a
pool of resources such as multiple GPUs on the same machine or multiple
machines with a single GPU. This begs the question: *How do we
efficiently distribute random search?*

In general, we distinguish between synchronous and asynchronous parallel
hyperparameter optimization (see :numref:`distributed_scheduling`). In
the synchronous setting, we wait for all concurrently running trials to
finish, before we start the next batch. Consider configuration spaces
that contain hyperparameters such as the number of filters or number of
layers of a deep neural network. Hyperparameter configurations that
contain a larger number of layers of filters will naturally take more
time to finish, and all other trials in the same batch will have to wait
at synchronisation points (grey area in
:numref:`distributed_scheduling`) before we can continue the
optimization process.

In the asynchronous setting we immediately schedule a new trial as soon
as resources become available. This will optimally exploit our
resources, since we can avoid any synchronisation overhead. For random
search, each new hyperparameter configuration is chosen independently of
all others, and in particular without exploiting observations from any
prior evaluation. This means we can trivially parallelize random search
asynchronously. This is not straight-forward with more sophisticated
methods that make decision based on previous observations (see
:numref:`sec_sh_async`). While we need access to more resources than
in the sequential setting, asynchronous random search exhibits a linear
speed-up, in that a certain performance is reached :math:`K` times
faster if :math:`K` trials can be run in parallel.

.. _distributed_scheduling:

.. figure:: ../img/distributed_scheduling.svg

   Distributing the hyperparameter optimization process either
   synchronously or asynchronously. Compared to the sequential setting,
   we can reduce the overall wall-clock time while keep the total
   compute constant. Synchronous scheduling might lead to idling workers
   in the case of stragglers.


In this notebook, we will look at asynchronous random search that, where
trials are executed in multiple python processes on the same machine.
Distributed job scheduling and execution is difficult to implement from
scratch. We will use *Syne Tune* :cite:`salinas-automl22`, which
provides us with a simple interface for asynchronous HPO. Syne Tune is
designed to be run with different execution back-ends, and the
interested reader is invited to study its simple APIs in order to learn
more about distributed HPO.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import logging
    from d2l import mlx as d2l
    
    logging.basicConfig(level=logging.INFO)
    from syne_tune import StoppingCriterion, Tuner
    from syne_tune.backend import PythonBackend
    from syne_tune.config_space import loguniform, randint
    from syne_tune.experiments import load_experiment
    from syne_tune.optimizer.baselines import RandomSearch

Objective Function
------------------

First, we have to define a new objective function such that it now
returns the performance back to Syne Tune via the ``report`` callback.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def hpo_objective_lenet_synetune(learning_rate, batch_size, max_epochs):
        from syne_tune import Reporter
        from d2l import mlx as d2l
        model = d2l.LeNet(num_inputs=1, lr=learning_rate, num_classes=10)
        data = d2l.FashionMNIST(batch_size=batch_size)
        trainer = d2l.HPOTrainer(max_epochs=1)
        trainer.fit(model=model, data=data)
        report = Reporter()
        for epoch in range(1, max_epochs + 1):
            if epoch > 1:
                trainer.fit_epoch()
            val_err = trainer.validation_error()
            report(epoch=epoch, validation_error=float(val_err))

Note that the ``PythonBackend`` of Syne Tune requires dependencies to be
imported inside the function definition.

Asynchronous Scheduler
----------------------

First, we define the number of workers that evaluate trials
concurrently. We also need to specify how long we want to run random
search, by defining an upper limit on the total wall-clock time.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    n_workers = 2  # Needs to be <= the number of available GPUs
    
    max_wallclock_time = 12 * 60  # 12 minutes

Next, we state which metric we want to optimize and whether we want to
minimize or maximize this metric. Namely, ``metric`` needs to correspond
to the argument name passed to the ``report`` callback.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    # mode = "min"
    metric = "validation_error"

We use the configuration space from our previous example. In Syne Tune,
this dictionary can also be used to pass constant attributes to the
training script. We make use of this feature in order to pass
``max_epochs``. Moreover, we specify the first configuration to be
evaluated in ``initial_config``.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    config_space = {
        "learning_rate": loguniform(1e-2, 1),
        "batch_size": randint(32, 256),
        "max_epochs": 10,
    }
    initial_config = {
        "learning_rate": 0.1,
        "batch_size": 128,
    }

Next, we need to specify the back-end for job executions. Here we just
consider the distribution on a local machine where parallel jobs are
executed as sub-processes. However, for large scale HPO, we could run
this also on a cluster or cloud environment, where each trial consumes a
full instance.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    trial_backend = PythonBackend(
        tune_function=hpo_objective_lenet_synetune,
        config_space=config_space,
    )

We can now create the scheduler for asynchronous random search, which is
similar in behaviour to our ``BasicScheduler`` from
:numref:`sec_api_hpo`.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    scheduler = RandomSearch(
        config_space,
        metrics=["validation_error"],
        do_minimize=True,
        points_to_evaluate=[initial_config],
    )

Syne Tune also features a ``Tuner``, where the main experiment loop and
bookkeeping is centralized, and interactions between scheduler and
back-end are mediated.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)
    
    tuner = Tuner(
        trial_backend=trial_backend,
        scheduler=scheduler,
        stop_criterion=stop_criterion,
        n_workers=n_workers,
        print_update_interval=int(max_wallclock_time * 0.6),
    )

Let us run our distributed HPO experiment. According to our stopping
criterion, it will run for about 12 minutes.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    tuner.run()


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    INFO:syne_tune.tuner:results of trials will be saved on /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677
    INFO:root:Error launching /usr/bin/nvidia-smi, no GPU could be detected.
    INFO:syne_tune.backend.local_backend:Detected 0 GPUs
    WARNING:syne_tune.backend.local_backend:num_gpus_per_trial = 1 is too large, reducing to 0
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.1 --batch_size 128 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/0/checkpoints
    INFO:syne_tune.tuner:(trial 0) - scheduled config {'learning_rate': 0.1, 'batch_size': 128, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.785419139652293 --batch_size 253 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/1/checkpoints
    INFO:syne_tune.tuner:(trial 1) - scheduled config {'learning_rate': 0.785419139652293, 'batch_size': 253, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 1 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.015451559930309012 --batch_size 55 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/2/checkpoints
    INFO:syne_tune.tuner:(trial 2) - scheduled config {'learning_rate': 0.015451559930309012, 'batch_size': 55, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 0 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.314703831688577 --batch_size 132 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/3/checkpoints
    INFO:syne_tune.tuner:(trial 3) - scheduled config {'learning_rate': 0.314703831688577, 'batch_size': 132, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 3 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.02832486312228176 --batch_size 86 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/4/checkpoints
    INFO:syne_tune.tuner:(trial 4) - scheduled config {'learning_rate': 0.02832486312228176, 'batch_size': 86, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 2 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.3114746598543795 --batch_size 220 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/5/checkpoints
    INFO:syne_tune.tuner:(trial 5) - scheduled config {'learning_rate': 0.3114746598543795, 'batch_size': 220, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 4 completed.
    INFO:syne_tune.tuner:Trial trial_id 5 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.14734507957489268 --batch_size 220 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/6/checkpoints
    INFO:syne_tune.tuner:(trial 6) - scheduled config {'learning_rate': 0.14734507957489268, 'batch_size': 220, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.37475164983438675 --batch_size 159 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/7/checkpoints
    INFO:syne_tune.tuner:(trial 7) - scheduled config {'learning_rate': 0.37475164983438675, 'batch_size': 159, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 6 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.4994060435597754 --batch_size 87 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/8/checkpoints
    INFO:syne_tune.tuner:(trial 8) - scheduled config {'learning_rate': 0.4994060435597754, 'batch_size': 87, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 7 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.06238492357004281 --batch_size 54 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/9/checkpoints
    INFO:syne_tune.tuner:(trial 9) - scheduled config {'learning_rate': 0.06238492357004281, 'batch_size': 54, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 8 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.14754101174871392 --batch_size 153 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/10/checkpoints
    INFO:syne_tune.tuner:(trial 10) - scheduled config {'learning_rate': 0.14754101174871392, 'batch_size': 153, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 9 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.42188880580044036 --batch_size 88 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/11/checkpoints
    INFO:syne_tune.tuner:(trial 11) - scheduled config {'learning_rate': 0.42188880580044036, 'batch_size': 88, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 10 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.2993200275939439 --batch_size 86 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/12/checkpoints
    INFO:syne_tune.tuner:(trial 12) - scheduled config {'learning_rate': 0.2993200275939439, 'batch_size': 86, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 11 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.19147449863618432 --batch_size 91 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/13/checkpoints
    INFO:syne_tune.tuner:(trial 13) - scheduled config {'learning_rate': 0.19147449863618432, 'batch_size': 91, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 12 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.26059001016457334 --batch_size 72 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/14/checkpoints
    INFO:syne_tune.tuner:(trial 14) - scheduled config {'learning_rate': 0.26059001016457334, 'batch_size': 72, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 13 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.964713800900415 --batch_size 47 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/15/checkpoints
    INFO:syne_tune.tuner:(trial 15) - scheduled config {'learning_rate': 0.964713800900415, 'batch_size': 47, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 14 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.10696632399613491 --batch_size 248 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/16/checkpoints
    INFO:syne_tune.tuner:(trial 16) - scheduled config {'learning_rate': 0.10696632399613491, 'batch_size': 248, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 15 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.5917570853739808 --batch_size 65 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/17/checkpoints
    INFO:syne_tune.tuner:(trial 17) - scheduled config {'learning_rate': 0.5917570853739808, 'batch_size': 65, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 16 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.04447784402047958 --batch_size 80 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/18/checkpoints
    INFO:syne_tune.tuner:(trial 18) - scheduled config {'learning_rate': 0.04447784402047958, 'batch_size': 80, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 17 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.20403568510229048 --batch_size 108 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/19/checkpoints
    INFO:syne_tune.tuner:(trial 19) - scheduled config {'learning_rate': 0.20403568510229048, 'batch_size': 108, 'max_epochs': 10}
    INFO:syne_tune.tuner:tuning status (last metric is reported)
     trial_id     status  iter  learning_rate  batch_size  max_epochs  epoch  validation_error  worker-time
            0  Completed    10       0.100000         128          10   10.0          0.269680    29.100968
            1  Completed    10       0.785419         253          10   10.0          0.211973    24.808257
            2  Completed    10       0.015452          55          10   10.0          0.746909    43.373880
            3  Completed    10       0.314704         132          10   10.0          0.198006    31.834912
            4  Completed    10       0.028325          86          10   10.0          0.469100    34.693813
            5  Completed    10       0.311475         220          10   10.0          0.238202    26.783766
            6  Completed    10       0.147345         220          10   10.0          0.287549    25.384183
            7  Completed    10       0.374752         159          10   10.0          0.233063    27.162082
            8  Completed    10       0.499406          87          10   10.0          0.155126    37.493749
            9  Completed    10       0.062385          54          10   10.0          0.278216    43.484688
           10  Completed    10       0.147541         153          10   10.0          0.262952    27.690224
           11  Completed    10       0.421889          88          10   10.0          0.152469    32.553498
           12  Completed    10       0.299320          86          10   10.0          0.178825    33.563370
           13  Completed    10       0.191474          91          10   10.0          0.226708    33.486506
           14  Completed    10       0.260590          72          10   10.0          0.189823    39.855617
           15  Completed    10       0.964714          47          10   10.0          0.121458    46.087984
           16  Completed    10       0.106966         248          10   10.0          0.350325    23.626774
           17  Completed    10       0.591757          65          10   10.0          0.262156    41.274113
           18 InProgress     9       0.044478          80          10    9.0          0.335800    33.564189
           19 InProgress     0       0.204036         108          10      -                 -            -
    2 trials running, 18 finished (18 until the end), 436.03s wallclock-time
    
    INFO:syne_tune.tuner:Trial trial_id 18 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.7649756579068647 --batch_size 212 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/20/checkpoints
    INFO:syne_tune.tuner:(trial 20) - scheduled config {'learning_rate': 0.7649756579068647, 'batch_size': 212, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 19 completed.
    INFO:syne_tune.tuner:Trial trial_id 20 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.0988395141460621 --batch_size 251 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/21/checkpoints
    INFO:syne_tune.tuner:(trial 21) - scheduled config {'learning_rate': 0.0988395141460621, 'batch_size': 251, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.701409313060209 --batch_size 90 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/22/checkpoints
    INFO:syne_tune.tuner:(trial 22) - scheduled config {'learning_rate': 0.701409313060209, 'batch_size': 90, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 21 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.7311914189829157 --batch_size 224 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/23/checkpoints
    INFO:syne_tune.tuner:(trial 23) - scheduled config {'learning_rate': 0.7311914189829157, 'batch_size': 224, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 22 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.014936836616036158 --batch_size 240 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/24/checkpoints
    INFO:syne_tune.tuner:(trial 24) - scheduled config {'learning_rate': 0.014936836616036158, 'batch_size': 240, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 23 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.13976762587971506 --batch_size 96 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/25/checkpoints
    INFO:syne_tune.tuner:(trial 25) - scheduled config {'learning_rate': 0.13976762587971506, 'batch_size': 96, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 24 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.6377240017626923 --batch_size 165 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/26/checkpoints
    INFO:syne_tune.tuner:(trial 26) - scheduled config {'learning_rate': 0.6377240017626923, 'batch_size': 165, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 25 completed.
    INFO:syne_tune.tuner:Trial trial_id 26 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.01286225998599121 --batch_size 108 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/27/checkpoints
    INFO:syne_tune.tuner:(trial 27) - scheduled config {'learning_rate': 0.01286225998599121, 'batch_size': 108, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.02014490729340573 --batch_size 124 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/28/checkpoints
    INFO:syne_tune.tuner:(trial 28) - scheduled config {'learning_rate': 0.02014490729340573, 'batch_size': 124, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 27 completed.
    INFO:syne_tune.tuner:Trial trial_id 28 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.1333083293327977 --batch_size 68 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/29/checkpoints
    INFO:syne_tune.tuner:(trial 29) - scheduled config {'learning_rate': 0.1333083293327977, 'batch_size': 68, 'max_epochs': 10}
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.2879408868000836 --batch_size 102 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/30/checkpoints
    INFO:syne_tune.tuner:(trial 30) - scheduled config {'learning_rate': 0.2879408868000836, 'batch_size': 102, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 30 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.011605333152295885 --batch_size 68 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/31/checkpoints
    INFO:syne_tune.tuner:(trial 31) - scheduled config {'learning_rate': 0.011605333152295885, 'batch_size': 68, 'max_epochs': 10}
    INFO:syne_tune.tuner:Trial trial_id 29 completed.
    INFO:syne_tune.backend.local_backend:running subprocess with command: /opt/anaconda3/envs/d2lbook-en/bin/python3.10 /opt/anaconda3/envs/d2lbook-en/lib/python3.10/site-packages/syne_tune/backend/python_backend/python_entrypoint.py --learning_rate 0.01484052260681797 --batch_size 163 --max_epochs 10 --tune_function_root /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/tune_function --tune_function_hash 2403b5305c7599b48033ce9ca0bc5baa --st_checkpoint_dir /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677/32/checkpoints
    INFO:syne_tune.tuner:(trial 32) - scheduled config {'learning_rate': 0.01484052260681797, 'batch_size': 163, 'max_epochs': 10}
    INFO:syne_tune.stopping_criterion:reaching max wallclock time (720), stopping there.
    INFO:syne_tune.tuner:Stopping trials that may still be running.
    INFO:syne_tune.tuner:Tuning finished, results of trials can be found on /Users/fangkeqiu/syne-tune/python-entrypoint-2025-08-11-07-47-51-677
    --------------------
    Resource summary (last result is reported):
     trial_id     status  iter  learning_rate  batch_size  max_epochs  epoch  validation_error  worker-time
            0  Completed    10       0.100000         128          10     10          0.269680    29.100968
            1  Completed    10       0.785419         253          10     10          0.211973    24.808257
            2  Completed    10       0.015452          55          10     10          0.746909    43.373880
            3  Completed    10       0.314704         132          10     10          0.198006    31.834912
            4  Completed    10       0.028325          86          10     10          0.469100    34.693813
            5  Completed    10       0.311475         220          10     10          0.238202    26.783766
            6  Completed    10       0.147345         220          10     10          0.287549    25.384183
            7  Completed    10       0.374752         159          10     10          0.233063    27.162082
            8  Completed    10       0.499406          87          10     10          0.155126    37.493749
            9  Completed    10       0.062385          54          10     10          0.278216    43.484688
           10  Completed    10       0.147541         153          10     10          0.262952    27.690224
           11  Completed    10       0.421889          88          10     10          0.152469    32.553498
           12  Completed    10       0.299320          86          10     10          0.178825    33.563370
           13  Completed    10       0.191474          91          10     10          0.226708    33.486506
           14  Completed    10       0.260590          72          10     10          0.189823    39.855617
           15  Completed    10       0.964714          47          10     10          0.121458    46.087984
           16  Completed    10       0.106966         248          10     10          0.350325    23.626774
           17  Completed    10       0.591757          65          10     10          0.262156    41.274113
           18  Completed    10       0.044478          80          10     10          0.330900    36.690488
           19  Completed    10       0.204036         108          10     10          0.207319    32.098498
           20  Completed    10       0.764976         212          10     10          0.331466    26.470002
           21  Completed    10       0.098840         251          10     10          0.358084    25.490762
           22  Completed    10       0.701409          90          10     10          0.145933    34.306699
           23  Completed    10       0.731191         224          10     10          0.170778    24.191342
           24  Completed    10       0.014937         240          10     10          0.900000    23.009057
           25  Completed    10       0.139768          96          10     10          0.223214    31.841101
           26  Completed    10       0.637724         165          10     10          0.171714    25.903314
           27  Completed    10       0.012862         108          10     10          0.899822    33.627085
           28  Completed    10       0.020145         124          10     10          0.899781    32.135524
           29  Completed    10       0.133308          68          10     10          0.214130    41.098702
           30  Completed    10       0.287941         102          10     10          0.205189    35.413799
           31 InProgress     7       0.011605          68          10      7          0.900636    26.102841
           32 InProgress     5       0.014841         163          10      5          0.899761    13.691636
    2 trials running, 31 finished (31 until the end), 721.94s wallclock-time
    
    validation_error: best 0.12145810759683173 for trial-id 15
    --------------------


The logs of all evaluated hyperparameter configurations are stored for
further analysis. At any time during the tuning job, we can easily get
the results obtained so far and plot the incumbent trajectory.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    d2l.set_figsize()
    tuning_experiment = load_experiment(tuner.name)
    tuning_experiment.plot()



.. figure:: output_rs-async_cdba80_19_0.svg


Visualize the Asynchronous Optimization Process
-----------------------------------------------

Below we visualize how the learning curves of every trial (each color in
the plot represents a trial) evolve during the asynchronous optimization
process. At any point in time, there are as many trials running
concurrently as we have workers. Once a trial finishes, we immediately
start the next trial, without waiting for the other trials to finish.
Idle time of workers is reduced to a minimum with asynchronous
scheduling.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    d2l.set_figsize([6, 2.5])
    results = tuning_experiment.results
    
    for trial_id in results.trial_id.unique():
        df = results[results["trial_id"] == trial_id]
        d2l.plt.plot(
            df["st_tuner_time"],
            df["validation_error"],
            marker="o"
        )
    
    d2l.plt.xlabel("wall-clock time")
    d2l.plt.ylabel("objective function")




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    Text(0, 0.5, 'objective function')




.. figure:: output_rs-async_cdba80_21_1.svg


Summary
-------

We can reduce the waiting time for random search substantially by
distribution trials across parallel resources. In general, we
distinguish between synchronous scheduling and asynchronous scheduling.
Synchronous scheduling means that we sample a new batch of
hyperparameter configurations once the previous batch finished. If we
have a stragglers - trials that takes more time to finish than other
trials - our workers need to wait at synchronization points.
Asynchronous scheduling evaluates a new hyperparameter configurations as
soon as resources become available, and, hence, ensures that all workers
are busy at any point in time. While random search is easy to distribute
asynchronously and does not require any change of the actual algorithm,
other methods require some additional modifications.

Exercises
---------

1. Consider the ``DropoutMLP`` model implemented in
   :numref:`sec_dropout`, and used in Exercise 1 of
   :numref:`sec_api_hpo`.

   1. Implement an objective function
      ``hpo_objective_dropoutmlp_synetune`` to be used with Syne Tune.
      Make sure that your function reports the validation error after
      every epoch.
   2. Using the setup of Exercise 1 in :numref:`sec_api_hpo`, compare
      random search to Bayesian optimization. If you use SageMaker, feel
      free to use Syne Tune’s benchmarking facilities in order to run
      experiments in parallel. Hint: Bayesian optimization is provided
      as ``syne_tune.optimizer.baselines.BayesianOptimization``.
   3. For this exercise, you need to run on an instance with at least 4
      CPU cores. For one of the methods used above (random search,
      Bayesian optimization), run experiments with ``n_workers=1``,
      ``n_workers=2``, ``n_workers=4``, and compare results (incumbent
      trajectories). At least for random search, you should observe
      linear scaling with respect to the number of workers. Hint: For
      robust results, you may have to average over several repetitions
      each.

2. *Advanced*. The goal of this exercise is to implement a new scheduler
   in Syne Tune.

   1. Create a virtual environment containing both the
      `d2lbook <https://github.com/d2l-ai/d2l-en/blob/master/INFO.md#installation-for-developers>`__
      and
      `syne-tune <https://syne-tune.readthedocs.io/en/latest/getting_started.html>`__
      sources.
   2. Implement the ``LocalSearcher`` from Exercise 2 in
      :numref:`sec_api_hpo` as a new searcher in Syne Tune. Hint: Read
      `this
      tutorial <https://syne-tune.readthedocs.io/en/latest/tutorials/developer/README.html>`__.
      Alternatively, you may follow this
      `example <https://syne-tune.readthedocs.io/en/latest/examples.html#launch-hpo-experiment-with-home-made-scheduler>`__.
   3. Compare your new ``LocalSearcher`` with ``RandomSearch`` on the
      ``DropoutMLP`` benchmark.
