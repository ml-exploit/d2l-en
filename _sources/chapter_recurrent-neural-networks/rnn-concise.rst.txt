
.. _sec_rnn-concise:

Concise Implementation of Recurrent Neural Networks
===================================================


Like most of our from-scratch implementations,
:numref:`sec_rnn-scratch` was designed to provide insight into how
each component works. But when you are using RNNs every day or writing
production code, you will want to rely more on libraries that cut down
on both implementation time (by supplying library code for common models
and functions) and computation time (by optimizing the heck out of these
library implementations). This section will show you how to implement
the same language model more efficiently using the high-level API
provided by your deep learning framework. We begin, as before, by
loading *The Time Machine* dataset.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import mlx.core as mx
    import mlx.nn as nn
    import numpy as np
    from d2l import mlx as d2l

Defining the Model
------------------

We define the following class using the RNN implemented by high-level
APIs.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class RNN(d2l.Module):  #@save
        """The RNN model implemented with high-level APIs."""
        def __init__(self, num_inputs, num_hiddens):
            super().__init__()
            self.save_hyperparameters()
            self.rnn = d2l.RNNScratch(num_inputs, num_hiddens)
    
        def __call__(self, inputs, H=None):
            return self.rnn(inputs, H)

Inheriting from the ``RNNLMScratch`` class in
:numref:`sec_rnn-scratch`, the following ``RNNLM`` class defines a
complete RNN-based language model. Note that we need to create a
separate fully connected output layer.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class RNNLM(d2l.RNNLMScratch):  #@save
        """The RNN-based language model implemented with high-level APIs."""
        def init_params(self, num_inputs):
            self.linear = nn.Linear(num_inputs, self.vocab_size)
    
        def output_layer(self, hiddens):
            outputs = [self.linear(H) for H in hiddens]
            return mx.stack(outputs, axis=1)

Training and Predicting
-----------------------

Before training the model, letâ€™s make a prediction with a model
initialized with random weights. Given that we have not trained the
network, it will generate nonsensical predictions.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    data = d2l.TimeMachine(batch_size=1024, num_steps=32)
    rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)
    model = RNNLM(rnn=rnn, num_inputs=32, vocab_size=len(data.vocab), lr=1)
    model.predict('it has', 20, data.vocab)




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    'it hasllllllllllllllllllll'



Next, we train our model, leveraging the high-level API.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)



.. figure:: output_rnn-concise_bfefb3_9_0.svg


Compared with :numref:`sec_rnn-scratch`, this model achieves
comparable perplexity, but runs faster due to the optimized
implementations. As before, we can generate predicted tokens following
the specified prefix string.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    model.predict('it has', 20, data.vocab, d2l.try_gpu())




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    'it has of the the the the '



Summary
-------

High-level APIs in deep learning frameworks provide implementations of
standard RNNs. These libraries help you to avoid wasting time
reimplementing standard models. Moreover, framework implementations are
often highly optimized, leading to significant (computational)
performance gains when compared with implementations from scratch.

Exercises
---------

1. Can you make the RNN model overfit using the high-level APIs?
2. Implement the autoregressive model of :numref:`sec_sequence` using
   an RNN.
