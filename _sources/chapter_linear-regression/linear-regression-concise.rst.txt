
.. _sec_linear_concise:

Concise Implementation of Linear Regression
===========================================


Deep learning has witnessed a sort of Cambrian explosion over the past
decade. The sheer number of techniques, applications and algorithms by
far surpasses the progress of previous decades. This is due to a
fortuitous combination of multiple factors, one of which is the powerful
free tools offered by a number of open-source deep learning frameworks.
Theano :cite:`Bergstra.Breuleux.Bastien.ea.2010`, DistBelief
:cite:`Dean.Corrado.Monga.ea.2012`, and Caffe
:cite:`Jia.Shelhamer.Donahue.ea.2014` arguably represent the first
generation of such models that found widespread adoption. In contrast to
earlier (seminal) works like SN2 (Simulateur Neuristique)
:cite:`Bottou.Le-Cun.1988`, which provided a Lisp-like programming
experience, modern frameworks offer automatic differentiation and the
convenience of Python. These frameworks allow us to automate and
modularize the repetitive work of implementing gradient-based learning
algorithms.

In :numref:`sec_linear_scratch`, we relied only on (i) tensors for
data storage and linear algebra; and (ii) automatic differentiation for
calculating gradients. In practice, because data iterators, loss
functions, optimizers, and neural network layers are so common, modern
libraries implement these components for us as well. In this section, we
will show you how to implement the linear regression model from
:numref:`sec_linear_scratch` concisely by using high-level APIs of
deep learning frameworks.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import mlx.core as mx
    import mlx.nn as nn
    import mlx.optimizers as optim
    import numpy as np
    from d2l import mlx as d2l

Defining the Model
------------------

When we implemented linear regression from scratch in
:numref:`sec_linear_scratch`, we defined our model parameters
explicitly and coded up the calculations to produce output using basic
linear algebra operations. You *should* know how to do this. But once
your models get more complex, and once you have to do this nearly every
day, you will be glad of the assistance. The situation is similar to
coding up your own blog from scratch. Doing it once or twice is
rewarding and instructive, but you would be a lousy web developer if you
spent a month reinventing the wheel.

For standard operations, we can use a framework’s predefined layers,
which allow us to focus on the layers used to construct the model rather
than worrying about their implementation. Recall the architecture of a
single-layer network as described in :numref:`fig_single_neuron`. The
layer is called *fully connected*, since each of its inputs is connected
to each of its outputs by means of a matrix–vector multiplication.

In MLX, the fully connected layer is defined in ``Linear`` classes
(available since version 0.15.0). It asks for how many inputs go into
this layer. Specifying input shapes is inconvenient and may require
nontrivial calculations (such as in convolutional layers). Thus, for
simplicity, we will use such “lazy” layers whenever we can.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class LinearRegression(d2l.Module):  #@save
        """The linear regression model implemented with high-level APIs."""
        def __init__(self, num_inputs, lr):
            super().__init__()
            self.save_hyperparameters()
            self.net = nn.Linear(input_dims=num_inputs, output_dims=1, bias=True)
            weight_fn = nn.init.normal(mean=0.0, std=0.01)
            bias_fn = nn.init.constant(0.0)
            self.net.weight = weight_fn(self.net.parameters()["weight"])
            self.net.bias = bias_fn(self.net.parameters()["bias"])

In the ``forward`` method we just invoke the built-in ``__call__``
method of the predefined layers to compute the outputs.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    @d2l.add_to_class(LinearRegression)  #@save
    def __call__(self, X):
        return self.net(X)

Defining the Loss Function
--------------------------

The ``MSELoss`` class computes the mean squared error (without the
:math:`1/2` factor in :eq:`eq_mse`). By default, ``MSELoss``
returns the average loss over examples. It is faster (and easier to use)
than implementing our own.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    @d2l.add_to_class(LinearRegression)  #@save
    def loss(self, y_hat, y):
        fn = nn.losses.mse_loss
        return fn(y_hat, y)

Defining the Optimization Algorithm
-----------------------------------

Minibatch SGD is a standard tool for optimizing neural networks and thus
PyTorch supports it alongside a number of variations on this algorithm
in the ``optim`` module. When we instantiate an ``SGD`` instance, we
specify the learning rate (``self.lr``) required by our optimization
algorithm.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    @d2l.add_to_class(LinearRegression)  #@save
    def configure_optimizers(self):
        return optim.SGD(learning_rate=self.lr)

Training
--------

You might have noticed that expressing our model through high-level APIs
of a deep learning framework requires fewer lines of code. We did not
have to allocate parameters individually, define our loss function, or
implement minibatch SGD. Once we start working with much more complex
models, the advantages of the high-level API will grow considerably.

Now that we have all the basic pieces in place, the training loop itself
is the same as the one we implemented from scratch. So we just call the
``fit`` method (introduced in :numref:`oo-design-training`), which
relies on the implementation of the ``fit_epoch`` method in
:numref:`sec_linear_scratch`, to train our model.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    model = LinearRegression(num_inputs=2, lr=0.03)
    data = d2l.SyntheticRegressionData(w=mx.array([2, -3.4]), b=4.2)
    trainer = d2l.Trainer(max_epochs=3)
    trainer.fit(model, data)



.. figure:: output_linear-regression-concise_13e119_14_0.svg


Below, we compare the model parameters learned by training on finite
data and the actual parameters that generated our dataset. To access
parameters, we access the weights and bias of the layer that we need. As
in our implementation from scratch, note that our estimated parameters
are close to their true counterparts.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    @d2l.add_to_class(LinearRegression)  #@save
    def get_w_b(self):
        return (self.net.weight, self.net.bias)
    
    w, b = model.get_w_b()

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    print(f'error in estimating w: {data.w - w.reshape(data.w.shape)}')
    print(f'error in estimating b: {data.b - b}')


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    error in estimating w: array([0.00405765, -0.0094018], dtype=float32)
    error in estimating b: array([0.0103469], dtype=float32)


Summary
-------

This section contains the first implementation of a deep network (in
this book) to tap into the conveniences afforded by modern deep learning
frameworks, such as MXNet :cite:`Chen.Li.Li.ea.2015`, JAX
:cite:`Frostig.Johnson.Leary.2018`, PyTorch
:cite:`Paszke.Gross.Massa.ea.2019`, and Tensorflow
:cite:`Abadi.Barham.Chen.ea.2016`. We used framework defaults for
loading data, defining a layer, a loss function, an optimizer and a
training loop. Whenever the framework provides all necessary features,
it is generally a good idea to use them, since the library
implementations of these components tend to be heavily optimized for
performance and properly tested for reliability. At the same time, try
not to forget that these modules *can* be implemented directly. This is
especially important for aspiring researchers who wish to live on the
leading edge of model development, where you will be inventing new
components that cannot possibly exist in any current library.

Exercises
---------

1. How would you need to change the learning rate if you replace the
   aggregate loss over the minibatch with an average over the loss on
   the minibatch?
2. Review the framework documentation to see which loss functions are
   provided. In particular, replace the squared loss with Huber’s robust
   loss function. That is, use the loss function

   .. math:: l(y,y') = \begin{cases}|y-y'| -\frac{\sigma}{2} & \textrm{ if } |y-y'| > \sigma \\ \frac{1}{2 \sigma} (y-y')^2 & \textrm{ otherwise}\end{cases}
3. How do you access the gradient of the weights of the model?
4. What is the effect on the solution if you change the learning rate
   and the number of epochs? Does it keep on improving?
5. How does the solution change as you vary the amount of data
   generated?

   1. Plot the estimation error for
      :math:`\hat{\mathbf{w}} - \mathbf{w}` and :math:`\hat{b} - b` as a
      function of the amount of data. Hint: increase the amount of data
      logarithmically rather than linearly, i.e., 5, 10, 20, 50, …,
      10,000 rather than 1000, 2000, …, 10,000.
   2. Why is the suggestion in the hint appropriate?
