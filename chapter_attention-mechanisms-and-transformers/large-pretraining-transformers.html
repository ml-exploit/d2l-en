<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>11.9. Large-Scale Pretraining with Transformers &#8212; Dive into Deep Learning 1.0.3 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12. Optimization Algorithms" href="../chapter_optimization/index.html" />
    <link rel="prev" title="11.8. Transformers for Vision" href="vision-transformer.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">11. </span>Attention Mechanisms and Transformers</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">11.9. </span>Large-Scale Pretraining with Transformers</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="###_ALTERNATE_VERSION_BASE_LINK_###">
                  <i class="fas fa-book"></i>
                  MLX
              </a>
          
              <a  class="mdl-navigation__link" href="###_CURRENT_VERSION_BASE_LINK_###/d2l-en.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PyTorch
              </a>
          
              <a  class="mdl-navigation__link" href="###_CURRENT_VERSION_BASE_LINK_###/d2l-en-mxnet.pdf">
                  <i class="fas fa-file-pdf"></i>
                  MXNet
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.zip">
                  <i class="fab fa-python"></i>
                  Notebooks
              </a>
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai">
                  <i class="fas fa-user-graduate"></i>
                  Courses
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/d2l-ai/d2l-en">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://zh.d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  中文版
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Dive into Deep Learning"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">Notation</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. Data Manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. Probability and Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-regression/index.html">3. Linear Neural Networks for Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression.html">3.1. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/oo-design.html">3.2. Object-Oriented Design for Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/synthetic-regression-data.html">3.3. Synthetic Regression Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-scratch.html">3.4. Linear Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-concise.html">3.5. Concise Implementation of Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/generalization.html">3.6. Generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/weight-decay.html">3.7. Weight Decay</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-classification/index.html">4. Linear Neural Networks for Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression.html">4.1. Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/image-classification-dataset.html">4.2. The Image Classification Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/classification.html">4.3. The Base Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-scratch.html">4.4. Softmax Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-concise.html">4.5. Concise Implementation of Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/generalization-classification.html">4.6. Generalization in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html">4.7. Environment and Distribution Shift</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">5. Multilayer Perceptrons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">5.1. Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-implementation.html">5.2. Implementation of Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">5.3. Forward Propagation, Backward Propagation, and Computational Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">5.4. Numerical Stability and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/generalization-deep.html">5.5. Generalization in Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">5.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">5.7. Predicting House Prices on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_builders-guide/index.html">6. Builders’ Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/model-construction.html">6.1. Layers and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/parameters.html">6.2. Parameter Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/init-param.html">6.3. Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/custom-layer.html">6.4. Custom Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/read-write.html">6.5. File I/O</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/use-gpu.html">6.6. GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">7. Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">7.1. From Fully Connected Layers to Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">7.2. Convolutions for Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">7.3. Padding and Stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">7.4. Multiple Input and Multiple Output Channels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">7.5. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">7.6. Convolutional Neural Networks (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">8. Modern Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">8.1. Deep Convolutional Neural Networks (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">8.2. Multi-Branch Networks (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">8.3. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">8.4. Residual Networks (ResNet) and ResNeXt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">8.5. Densely Connected Networks (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">9. Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">9.1. Working with Sequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-sequence.html">9.2. Converting Raw Text into Sequence Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html">9.3. Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">9.4. Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">9.5. Recurrent Neural Network Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">9.6. Concise Implementation of Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">9.7. Backpropagation Through Time</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">10. Modern Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">10.1. Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">10.2. Gated Recurrent Units (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">10.3. Deep Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">10.4. Machine Translation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">10.5. The Encoder–Decoder Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">10.6. Sequence-to-Sequence Learning for Machine Translation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">10.7. Beam Search</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">11. Attention Mechanisms and Transformers</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="queries-keys-values.html">11.1. Queries, Keys, and Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="attention-pooling.html">11.2. Attention Pooling by Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="attention-scoring-functions.html">11.3. Attention Scoring Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="bahdanau-attention.html">11.4. The Bahdanau Attention Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="multihead-attention.html">11.5. Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="self-attention-and-positional-encoding.html">11.6. Self-Attention and Positional Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformer.html">11.7. The Transformer Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="vision-transformer.html">11.8. Transformers for Vision</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11.9. Large-Scale Pretraining with Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">12. Optimization Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity.html">12.2. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd.html">12.3. Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd.html">12.4. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd.html">12.5. Minibatch Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum.html">12.6. Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad.html">12.7. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop.html">12.8. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta.html">12.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam.html">12.10. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler.html">12.11. Learning Rate Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">13. Computer Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">13.1. Image Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">13.2. Fine-Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">13.3. Object Detection and Bounding Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">13.4. Anchor Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">13.5. Multiscale Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">13.6. The Object Detection Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">13.7. Single Shot Multibox Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">13.8. Region-based CNNs (R-CNNs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">13.9. Semantic Segmentation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">13.10. Transposed Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">13.11. Fully Convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">13.12. Neural Style Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">13.13. Image Classification (CIFAR-10) on Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">14. Natural Language Processing: Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">14.1. Word Embedding (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">14.2. Approximate Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">14.3. The Dataset for Pretraining Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">14.4. Pretraining word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">14.5. Word Embedding with Global Vectors (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">14.6. Subword Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">14.7. Word Similarity and Analogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">14.8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">14.9. The Dataset for Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">14.10. Pretraining BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">15. Natural Language Processing: Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">15.1. Sentiment Analysis and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">15.2. Sentiment Analysis: Using Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">15.3. Natural Language Inference and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">15.4. Natural Language Inference: Using Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">15.5. Fine-Tuning BERT for Sequence-Level and Token-Level Applications</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement-learning/index.html">16. Reinforcement Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/mdp.html">16.1. Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/value-iter.html">16.2. Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/qlearning.html">16.3. Q-Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gaussian-processes/index.html">17. Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-intro.html">17.1. Introduction to Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-priors.html">17.2. Gaussian Process Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-inference.html">17.3. Gaussian Process Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hyperparameter-optimization/index.html">18. Hyperparameter Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-intro.html">18.1. What Is Hyperparameter Optimization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-api.html">18.2. Hyperparameter Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/rs-async.html">18.3. Asynchronous Random Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-intro.html">18.4. Multi-Fidelity Hyperparameter Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-async.html">18.5. Asynchronous Successive Halving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">19. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">19.1. Utility Functions and Classes</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Dive into Deep Learning"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">Notation</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. Data Manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. Probability and Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-regression/index.html">3. Linear Neural Networks for Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression.html">3.1. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/oo-design.html">3.2. Object-Oriented Design for Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/synthetic-regression-data.html">3.3. Synthetic Regression Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-scratch.html">3.4. Linear Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-concise.html">3.5. Concise Implementation of Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/generalization.html">3.6. Generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/weight-decay.html">3.7. Weight Decay</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-classification/index.html">4. Linear Neural Networks for Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression.html">4.1. Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/image-classification-dataset.html">4.2. The Image Classification Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/classification.html">4.3. The Base Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-scratch.html">4.4. Softmax Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-concise.html">4.5. Concise Implementation of Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/generalization-classification.html">4.6. Generalization in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html">4.7. Environment and Distribution Shift</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">5. Multilayer Perceptrons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">5.1. Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-implementation.html">5.2. Implementation of Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">5.3. Forward Propagation, Backward Propagation, and Computational Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">5.4. Numerical Stability and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/generalization-deep.html">5.5. Generalization in Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">5.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">5.7. Predicting House Prices on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_builders-guide/index.html">6. Builders’ Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/model-construction.html">6.1. Layers and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/parameters.html">6.2. Parameter Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/init-param.html">6.3. Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/custom-layer.html">6.4. Custom Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/read-write.html">6.5. File I/O</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/use-gpu.html">6.6. GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">7. Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">7.1. From Fully Connected Layers to Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">7.2. Convolutions for Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">7.3. Padding and Stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">7.4. Multiple Input and Multiple Output Channels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">7.5. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">7.6. Convolutional Neural Networks (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">8. Modern Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">8.1. Deep Convolutional Neural Networks (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">8.2. Multi-Branch Networks (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">8.3. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">8.4. Residual Networks (ResNet) and ResNeXt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">8.5. Densely Connected Networks (DenseNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">9. Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">9.1. Working with Sequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-sequence.html">9.2. Converting Raw Text into Sequence Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html">9.3. Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">9.4. Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">9.5. Recurrent Neural Network Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">9.6. Concise Implementation of Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">9.7. Backpropagation Through Time</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">10. Modern Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">10.1. Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">10.2. Gated Recurrent Units (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">10.3. Deep Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">10.4. Machine Translation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">10.5. The Encoder–Decoder Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">10.6. Sequence-to-Sequence Learning for Machine Translation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">10.7. Beam Search</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">11. Attention Mechanisms and Transformers</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="queries-keys-values.html">11.1. Queries, Keys, and Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="attention-pooling.html">11.2. Attention Pooling by Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="attention-scoring-functions.html">11.3. Attention Scoring Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="bahdanau-attention.html">11.4. The Bahdanau Attention Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="multihead-attention.html">11.5. Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="self-attention-and-positional-encoding.html">11.6. Self-Attention and Positional Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformer.html">11.7. The Transformer Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="vision-transformer.html">11.8. Transformers for Vision</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11.9. Large-Scale Pretraining with Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">12. Optimization Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity.html">12.2. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd.html">12.3. Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd.html">12.4. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd.html">12.5. Minibatch Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum.html">12.6. Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad.html">12.7. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop.html">12.8. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta.html">12.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam.html">12.10. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler.html">12.11. Learning Rate Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">13. Computer Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">13.1. Image Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">13.2. Fine-Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">13.3. Object Detection and Bounding Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">13.4. Anchor Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">13.5. Multiscale Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">13.6. The Object Detection Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">13.7. Single Shot Multibox Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">13.8. Region-based CNNs (R-CNNs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">13.9. Semantic Segmentation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">13.10. Transposed Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">13.11. Fully Convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">13.12. Neural Style Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">13.13. Image Classification (CIFAR-10) on Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">14. Natural Language Processing: Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">14.1. Word Embedding (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">14.2. Approximate Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">14.3. The Dataset for Pretraining Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">14.4. Pretraining word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">14.5. Word Embedding with Global Vectors (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">14.6. Subword Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">14.7. Word Similarity and Analogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">14.8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">14.9. The Dataset for Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">14.10. Pretraining BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">15. Natural Language Processing: Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">15.1. Sentiment Analysis and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">15.2. Sentiment Analysis: Using Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">15.3. Natural Language Inference and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">15.4. Natural Language Inference: Using Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">15.5. Fine-Tuning BERT for Sequence-Level and Token-Level Applications</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement-learning/index.html">16. Reinforcement Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/mdp.html">16.1. Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/value-iter.html">16.2. Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/qlearning.html">16.3. Q-Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gaussian-processes/index.html">17. Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-intro.html">17.1. Introduction to Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-priors.html">17.2. Gaussian Process Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-inference.html">17.3. Gaussian Process Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hyperparameter-optimization/index.html">18. Hyperparameter Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-intro.html">18.1. What Is Hyperparameter Optimization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-api.html">18.2. Hyperparameter Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/rs-async.html">18.3. Asynchronous Random Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-intro.html">18.4. Multi-Fidelity Hyperparameter Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-async.html">18.5. Asynchronous Successive Halving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">19. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">19.1. Utility Functions and Classes</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <section id="large-scale-pretraining-with-transformers">
<span id="sec-large-pretraining-transformers"></span><h1><span class="section-number">11.9. </span>Large-Scale Pretraining with Transformers<a class="headerlink" href="#large-scale-pretraining-with-transformers" title="Permalink to this heading">¶</a></h1>
<p>So far in our image classification and machine translation experiments,
models have been trained on datasets with input–output examples <em>from
scratch</em> to perform specific tasks. For example, a Transformer was
trained with English–French pairs (<a class="reference internal" href="transformer.html#sec-transformer"><span class="std std-numref">Section 11.7</span></a>) so that
this model can translate input English text into French. As a result,
each model becomes a <em>specific expert</em> that is sensitive to even a
slight shift in data distribution
(<a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html#sec-environment-and-distribution-shift"><span class="std std-numref">Section 4.7</span></a>). For better
generalized models, or even more competent <em>generalists</em> that can
perform multiple tasks with or without adaptation, <em>pretraining</em> models
on large data has been increasingly common.</p>
<p>Given larger data for pretraining, the Transformer architecture performs
better with an increased model size and training compute, demonstrating
superior <em>scaling</em> behavior. Specifically, performance of
Transformer-based language models scales as a power law with the amount
of model parameters, training tokens, and training compute
<span id="id1">()</span>. The scalability of Transformers is also
evidenced by the significantly boosted performance from larger vision
Transformers trained on larger data (discussed in
<a class="reference internal" href="vision-transformer.html#sec-vision-transformer"><span class="std std-numref">Section 11.8</span></a>). More recent success stories include
Gato, a <em>generalist</em> model that can play Atari, caption images, chat,
and act as a robot <span id="id2">()</span>. Gato is a single
Transformer that scales well when pretrained on diverse modalities,
including text, images, joint torques, and button presses. Notably, all
such multimodal data is serialized into a flat sequence of tokens, which
can be processed akin to text tokens (<a class="reference internal" href="transformer.html#sec-transformer"><span class="std std-numref">Section 11.7</span></a>) or
image patches (<a class="reference internal" href="vision-transformer.html#sec-vision-transformer"><span class="std std-numref">Section 11.8</span></a>) by Transformers.</p>
<p>Prior to the compelling success of pretraining Transformers for
multimodal data, Transformers were extensively pretrained with a wealth
of text. Originally proposed for machine translation, the Transformer
architecture in <a class="reference internal" href="transformer.html#fig-transformer"><span class="std std-numref">Fig. 11.7.1</span></a> consists of an encoder for
representing input sequences and a decoder for generating target
sequences. Primarily, Transformers can be used in three different modes:
<em>encoder-only</em>, <em>encoder–decoder</em>, and <em>decoder-only</em>. To conclude this
chapter, we will review these three modes and explain the scalability in
pretraining Transformers.</p>
<section id="encoder-only">
<h2><span class="section-number">11.9.1. </span>Encoder-Only<a class="headerlink" href="#encoder-only" title="Permalink to this heading">¶</a></h2>
<p>When only the Transformer encoder is used, a sequence of input tokens is
converted into the same number of representations that can be further
projected into output (e.g., classification). A Transformer encoder
consists of self-attention layers, where all input tokens attend to each
other. For example, vision Transformers depicted in <a class="reference internal" href="vision-transformer.html#fig-vit"><span class="std std-numref">Fig. 11.8.1</span></a>
are encoder-only, converting a sequence of input image patches into the
representation of a special “&lt;cls&gt;” token. Since this representation
depends on all input tokens, it is further projected into classification
labels. This design was inspired by an earlier encoder-only Transformer
pretrained on text: BERT (Bidirectional Encoder Representations from
Transformers) <span id="id3">()</span>.</p>
<section id="pretraining-bert">
<h3><span class="section-number">11.9.1.1. </span>Pretraining BERT<a class="headerlink" href="#pretraining-bert" title="Permalink to this heading">¶</a></h3>
<figure class="align-default" id="id63">
<span id="fig-bert-encoder-only"></span><img alt="../_images/bert-encoder-only.svg" src="../_images/bert-encoder-only.svg" /><figcaption>
<p><span class="caption-number">Fig. 11.9.1 </span><span class="caption-text">Left: Pretraining BERT with masked language modeling. Prediction of
the masked “love” token depends on all input tokens before and after
“love”. Right: Attention pattern in the Transformer encoder. Each
token along the vertical axis attends to all input tokens along the
horizontal axis.</span><a class="headerlink" href="#id63" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>BERT is pretrained on text sequences using <em>masked language modeling</em>:
input text with randomly masked tokens is fed into a Transformer encoder
to predict the masked tokens. As illustrated in
<a class="reference internal" href="#fig-bert-encoder-only"><span class="std std-numref">Fig. 11.9.1</span></a>, an original text sequence “I”,
“love”, “this”, “red”, “car” is prepended with the “&lt;cls&gt;” token, and
the “&lt;mask&gt;” token randomly replaces “love”; then the cross-entropy loss
between the masked token “love” and its prediction is to be minimized
during pretraining. Note that there is no constraint in the attention
pattern of Transformer encoders (right of
<a class="reference internal" href="#fig-bert-encoder-only"><span class="std std-numref">Fig. 11.9.1</span></a>) so all tokens can attend to each
other. Thus, prediction of “love” depends on input tokens before and
after it in the sequence. This is why BERT is a “bidirectional encoder”.
Without need for manual labeling, large-scale text data from books and
Wikipedia can be used for pretraining BERT.</p>
</section>
<section id="fine-tuning-bert">
<h3><span class="section-number">11.9.1.2. </span>Fine-Tuning BERT<a class="headerlink" href="#fine-tuning-bert" title="Permalink to this heading">¶</a></h3>
<p>The pretrained BERT can be <em>fine-tuned</em> to downstream encoding tasks
involving single text or text pairs. During fine-tuning, additional
layers can be added to BERT with randomized parameters: these parameters
and those pretrained BERT parameters will be <em>updated</em> to fit training
data of downstream tasks.</p>
<figure class="align-default" id="id64">
<span id="fig-bert-finetune-classification"></span><img alt="../_images/bert-finetune-classification.svg" src="../_images/bert-finetune-classification.svg" /><figcaption>
<p><span class="caption-number">Fig. 11.9.2 </span><span class="caption-text">Fine-tuning BERT for sentiment analysis.</span><a class="headerlink" href="#id64" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fig-bert-finetune-classification"><span class="std std-numref">Fig. 11.9.2</span></a> illustrates fine-tuning of
BERT for sentiment analysis. The Transformer encoder is a pretrained
BERT, which takes a text sequence as input and feeds the “&lt;cls&gt;”
representation (global representation of the input) into an additional
fully connected layer to predict the sentiment. During fine-tuning, the
cross-entropy loss between the prediction and the label on sentiment
analysis data is minimized via gradient-based algorithms, where the
additional layer is trained from scratch while pretrained parameters of
BERT are updated. BERT does more than sentiment analysis. The general
language representations learned by the 350-million-parameter BERT from
250 billion training tokens advanced the state of the art for natural
language tasks such as single text classification, text pair
classification or regression, text tagging, and question answering.</p>
<p>You may note that these downstream tasks include text pair
understanding. BERT pretraining has another loss for predicting whether
one sentence immediately follows the other. However, this loss was later
found to be less useful when pretraining RoBERTa, a BERT variant of the
same size, on 2000 billion tokens <span id="id4">()</span>. Other
derivatives of BERT improved model architectures or pretraining
objectives, such as ALBERT (enforcing parameter sharing)
<span id="id5">()</span>, SpanBERT (representing and predicting spans of
text) <span id="id6">()</span>, DistilBERT (lightweight via knowledge
distillation) <span id="id7">()</span>, and ELECTRA (replaced token
detection) <span id="id8">()</span>. Moreover, BERT inspired
Transformer pretraining in computer vision, such as with vision
Transformers <span id="id9">()</span>, Swin
Transformers <span id="id10">()</span>, and MAE (masked autoencoders)
<span id="id11">()</span>.</p>
</section>
</section>
<section id="encoderdecoder">
<h2><span class="section-number">11.9.2. </span>Encoder–Decoder<a class="headerlink" href="#encoderdecoder" title="Permalink to this heading">¶</a></h2>
<p>Since a Transformer encoder converts a sequence of input tokens into the
same number of output representations, the encoder-only mode cannot
generate a sequence of arbitrary length as in machine translation. As
originally proposed for machine translation, the Transformer
architecture can be outfitted with a decoder that autoregressively
predicts the target sequence of arbitrary length, token by token,
conditional on both encoder output and decoder output: (i) for
conditioning on encoder output, encoder–decoder cross-attention
(multi-head attention of decoder in <a class="reference internal" href="transformer.html#fig-transformer"><span class="std std-numref">Fig. 11.7.1</span></a>) allows
target tokens to attend to <em>all</em> input tokens; (ii) conditioning on
decoder output is achieved by a so-called <em>causal</em> attention (this name
is common in the literature but is misleading as it has little
connection to the proper study of causality) pattern (masked multi-head
attention of decoder in <a class="reference internal" href="transformer.html#fig-transformer"><span class="std std-numref">Fig. 11.7.1</span></a>), where any target
token can only attend to <em>past</em> and <em>present</em> tokens in the target
sequence.</p>
<p>To pretrain encoder–decoder Transformers beyond human-labeled machine
translation data, BART <span id="id12">()</span> and T5
<span id="id13">()</span> are two concurrently proposed
encoder–decoder Transformers pretrained on large-scale text corpora.
Both attempt to reconstruct original text in their pretraining
objectives, while the former emphasizes noising input (e.g., masking,
deletion, permutation, and rotation) and the latter highlights multitask
unification with comprehensive ablation studies.</p>
<section id="pretraining-t5">
<h3><span class="section-number">11.9.2.1. </span>Pretraining T5<a class="headerlink" href="#pretraining-t5" title="Permalink to this heading">¶</a></h3>
<p>As an example of the pretrained Transformer encoder–decoder, T5
(Text-to-Text Transfer Transformer) unifies many tasks as the same
text-to-text problem: for any task, the input of the encoder is a task
description (e.g., “Summarize”, “:”) followed by task input (e.g., a
sequence of tokens from an article), and the decoder predicts the task
output (e.g., a sequence of tokens summarizing the input article). To
perform as text-to-text, T5 is trained to generate some target text
conditional on input text.</p>
<figure class="align-default" id="id65">
<span id="fig-t5-encoder-decoder"></span><img alt="../_images/t5-encoder-decoder.svg" src="../_images/t5-encoder-decoder.svg" /><figcaption>
<p><span class="caption-number">Fig. 11.9.3 </span><span class="caption-text">Left: Pretraining T5 by predicting consecutive spans. The original
sentence is “I”, “love”, “this”, “red”, “car”, where “love” is
replaced by a special “&lt;X&gt;” token, and consecutive “red”, “car” are
replaced by a special “&lt;Y&gt;” token. The target sequence ends with a
special “&lt;Z&gt;” token. Right: Attention pattern in the Transformer
encoder–decoder. In the encoder self-attention (lower square), all
input tokens attend to each other; In the encoder–decoder
cross-attention (upper rectangle), each target token attends to all
input tokens; In the decoder self-attention (upper triangle), each
target token attends to present and past target tokens only (causal).</span><a class="headerlink" href="#id65" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>To obtain input and output from any original text, T5 is pretrained to
predict consecutive spans. Specifically, tokens from text are randomly
replaced by special tokens where each consecutive span is replaced by
the same special token. Consider the example in
<a class="reference internal" href="#fig-t5-encoder-decoder"><span class="std std-numref">Fig. 11.9.3</span></a>, where the original text is “I”,
“love”, “this”, “red”, “car”. Tokens “love”, “red”, “car” are randomly
replaced by special tokens. Since “red” and “car” are a consecutive
span, they are replaced by the same special token. As a result, the
input sequence is “I”, “&lt;X&gt;”, “this”, “&lt;Y&gt;”, and the target sequence is
“&lt;X&gt;”, “love”, “&lt;Y&gt;”, “red”, “car”, “&lt;Z&gt;”, where “&lt;Z&gt;” is another
special token marking the end. As shown in
<a class="reference internal" href="#fig-t5-encoder-decoder"><span class="std std-numref">Fig. 11.9.3</span></a>, the decoder has a causal attention
pattern to prevent itself from attending to future tokens during
sequence prediction.</p>
<p>In T5, predicting consecutive span is also referred to as reconstructing
corrupted text. With this objective, T5 is pretrained with 1000 billion
tokens from the C4 (Colossal Clean Crawled Corpus) data, which consists
of clean English text from the web <span id="id14">()</span>.</p>
</section>
<section id="fine-tuning-t5">
<h3><span class="section-number">11.9.2.2. </span>Fine-Tuning T5<a class="headerlink" href="#fine-tuning-t5" title="Permalink to this heading">¶</a></h3>
<p>Similar to BERT, T5 needs to be fine-tuned (updating T5 parameters) on
task-specific training data to perform this task. Major differences from
BERT fine-tuning include: (i) T5 input includes task descriptions; (ii)
T5 can generate sequences with arbitrary length with its Transformer
decoder; (iii) No additional layers are required.</p>
<figure class="align-default" id="id66">
<span id="fig-t5-finetune-summarization"></span><img alt="../_images/t5-finetune-summarization.svg" src="../_images/t5-finetune-summarization.svg" /><figcaption>
<p><span class="caption-number">Fig. 11.9.4 </span><span class="caption-text">Fine-tuning T5 for text summarization. Both the task description and
article tokens are fed into the Transformer encoder for predicting
the summary.</span><a class="headerlink" href="#id66" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fig-t5-finetune-summarization"><span class="std std-numref">Fig. 11.9.4</span></a> explains fine-tuning T5 using
text summarization as an example. In this downstream task, the task
description tokens “Summarize”, “:” followed by the article tokens are
input to the encoder.</p>
<p>After fine-tuning, the 11-billion-parameter T5 (T5-11B) achieved
state-of-the-art results on multiple encoding (e.g., classification) and
generation (e.g., summarization) benchmarks. Since released, T5 has been
extensively used in later research. For example, switch Transformers are
designed based on T5 to activate a subset of the parameters for better
computational efficiency <span id="id15">()</span>. In a text-to-image
model called Imagen, text is input to a frozen T5 encoder (T5-XXL) with
4.6 billion parameters <span id="id16">()</span>. The
photorealistic text-to-image examples in <a class="reference internal" href="#fig-imagen"><span class="std std-numref">Fig. 11.9.5</span></a> suggest
that the T5 encoder alone may effectively represent text even without
fine-tuning.</p>
<figure class="align-default" id="id67">
<span id="fig-imagen"></span><a class="reference internal image-reference" href="../_images/imagen.png"><img alt="../_images/imagen.png" src="../_images/imagen.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.9.5 </span><span class="caption-text">Text-to-image examples by the Imagen model, whose text encoder is
from T5 (figures taken from <span id="id17"></span>).</span><a class="headerlink" href="#id67" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="decoder-only">
<h2><span class="section-number">11.9.3. </span>Decoder-Only<a class="headerlink" href="#decoder-only" title="Permalink to this heading">¶</a></h2>
<p>We have reviewed encoder-only and encoder–decoder Transformers.
Alternatively, decoder-only Transformers remove the entire encoder and
the decoder sublayer with the encoder–decoder cross-attention from the
original encoder–decoder architecture depicted in
<a class="reference internal" href="transformer.html#fig-transformer"><span class="std std-numref">Fig. 11.7.1</span></a>. Nowadays, decoder-only Transformers have
been the <em>de facto</em> architecture in large-scale language modeling
(<a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html#sec-language-model"><span class="std std-numref">Section 9.3</span></a>), which leverages the world’s abundant
unlabeled text corpora via self-supervised learning.</p>
<section id="gpt-and-gpt-2">
<h3><span class="section-number">11.9.3.1. </span>GPT and GPT-2<a class="headerlink" href="#gpt-and-gpt-2" title="Permalink to this heading">¶</a></h3>
<p>Using language modeling as the training objective, the GPT (generative
pre-training) model chooses a Transformer decoder as its backbone
<span id="id18">()</span>.</p>
<figure class="align-default" id="id68">
<span id="fig-gpt-decoder-only"></span><img alt="../_images/gpt-decoder-only.svg" src="../_images/gpt-decoder-only.svg" /><figcaption>
<p><span class="caption-number">Fig. 11.9.6 </span><span class="caption-text">Left: Pretraining GPT with language modeling. The target sequence is
the input sequence shifted by one token. Both “&lt;bos&gt;” and “&lt;eos&gt;” are
special tokens marking the beginning and end of sequences,
respectively. Right: Attention pattern in the Transformer decoder.
Each token along the vertical axis attends to only its past tokens
along the horizontal axis (causal).</span><a class="headerlink" href="#id68" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Following the autoregressive language model training as described in
<a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html#subsec-partitioning-seqs"><span class="std std-numref">Section 9.3.3</span></a>, <a class="reference internal" href="#fig-gpt-decoder-only"><span class="std std-numref">Fig. 11.9.6</span></a>
illustrates GPT pretraining with a Transformer encoder, where the target
sequence is the input sequence shifted by one token. Note that the
attention pattern in the Transformer decoder enforces that each token
can only attend to its past tokens (future tokens cannot be attended to
because they have not yet been chosen).</p>
<p>GPT has 100 million parameters and needs to be fine-tuned for individual
downstream tasks. A much larger Transformer-decoder language model,
GPT-2, was introduced one year later <span id="id19">()</span>.
Compared with the original Transformer decoder in GPT, pre-normalization
(discussed in <a class="reference internal" href="vision-transformer.html#subsec-vit-encoder"><span class="std std-numref">Section 11.8.3</span></a>) and improved
initialization and weight-scaling were adopted in GPT-2. Pretrained on
40 GB of text, the 1.5-billion-parameter GPT-2 obtained the
state-of-the-art results on language modeling benchmarks and promising
results on multiple other tasks <em>without updating the parameters or
architecture</em>.</p>
</section>
<section id="gpt-3-and-beyond">
<h3><span class="section-number">11.9.3.2. </span>GPT-3 and Beyond<a class="headerlink" href="#gpt-3-and-beyond" title="Permalink to this heading">¶</a></h3>
<p>GPT-2 demonstrated potential of using the same language model for
multiple tasks without updating the model. This is more computationally
efficient than fine-tuning, which requires model updates via gradient
computation.</p>
<figure class="align-default" id="id69">
<span id="fig-gpt-3-xshot"></span><img alt="../_images/gpt-3-xshot.svg" src="../_images/gpt-3-xshot.svg" /><figcaption>
<p><span class="caption-number">Fig. 11.9.7 </span><span class="caption-text">Zero-shot, one-shot, few-shot in-context learning with language
models (Transformer decoders). No parameter update is needed.</span><a class="headerlink" href="#id69" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Before explaining the more computationally efficient use of language
models without parameter update, recall <a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html#sec-rnn-scratch"><span class="std std-numref">Section 9.5</span></a> that
a language model can be trained to generate a text sequence conditional
on some prefix text sequence. Thus, a pretrained language model may
generate the task output as a sequence <em>without parameter update</em>,
conditional on an input sequence with the task description,
task-specific input–output examples, and a prompt (task input). This
learning paradigm is called <em>in-context learning</em>
<span id="id20">()</span>, which can be further categorized into
<em>zero-shot</em>, <em>one-shot</em>, and <em>few-shot</em>, when there is no, one, and a
few task-specific input–output examples (<a class="reference internal" href="#fig-gpt-3-xshot"><span class="std std-numref">Fig. 11.9.7</span></a>).</p>
<figure class="align-default" id="id70">
<span id="fig-gpt3-xshot-scaling"></span><a class="reference internal image-reference" href="../_images/gpt3-xshot-scaling.png"><img alt="../_images/gpt3-xshot-scaling.png" src="../_images/gpt3-xshot-scaling.png" style="width: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.9.8 </span><span class="caption-text">Aggregate performance of GPT-3 for all 42 accuracy-denominated
benchmarks (caption adapted and figure taken from
<span id="id21"></span>).</span><a class="headerlink" href="#id70" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>These three settings were tested in GPT-3 <span id="id22">()</span>,
whose largest version uses data and model size about two orders of
magnitude larger than those in GPT-2. GPT-3 uses the same Transformer
decoder architecture as its direct predecessor GPT-2 except that
attention patterns (at the right in <a class="reference internal" href="#fig-gpt-decoder-only"><span class="std std-numref">Fig. 11.9.6</span></a>)
are sparser at alternating layers. Pretrained with 300 billion tokens,
GPT-3 performs better with larger model size, where few-shot performance
increases most rapidly (<a class="reference internal" href="#fig-gpt3-xshot-scaling"><span class="std std-numref">Fig. 11.9.8</span></a>).</p>
<p>The subsequent GPT-4 model did not fully disclose technical details in
its report <span id="id23">()</span>. By contrast with its predecessors,
GPT-4 is a large-scale, multimodal model that can take both text and
images as input and generate text output.</p>
</section>
</section>
<section id="scalability">
<h2><span class="section-number">11.9.4. </span>Scalability<a class="headerlink" href="#scalability" title="Permalink to this heading">¶</a></h2>
<p><a class="reference internal" href="#fig-gpt3-xshot-scaling"><span class="std std-numref">Fig. 11.9.8</span></a> empirically demonstrates scalability
of Transformers in the GPT-3 language model. For language modeling, more
comprehensive empirical studies on the scalability of Transformers have
led researchers to see promise in training larger Transformers with more
data and compute <span id="id24">()</span>.</p>
<figure class="align-default" id="id71">
<span id="fig-scaling-power-law3"></span><a class="reference internal image-reference" href="../_images/scaling-power-law.png"><img alt="../_images/scaling-power-law.png" src="../_images/scaling-power-law.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.9.9 </span><span class="caption-text">Transformer language model performance improves smoothly as we
increase the model size, dataset size, and amount of compute used for
training. For optimal performance all three factors must be scaled up
in tandem. Empirical performance has a power-law relationship with
each individual factor when not bottlenecked by the other two
(caption adapted and figure taken from <span id="id25"></span>).</span><a class="headerlink" href="#id71" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>As shown in <a class="reference internal" href="#fig-scaling-power-law3"><span class="std std-numref">Fig. 11.9.9</span></a>, <em>power-law scaling</em> can
be observed in the performance with respect to the model size (number of
parameters, excluding embedding layers), dataset size (number of
training tokens), and amount of training compute (PetaFLOP/s-days,
excluding embedding layers). In general, increasing all these three
factors in tandem leads to better performance. However, <em>how</em> to
increase them in tandem still remains a matter of debate
<span id="id26">()</span>.</p>
<figure class="align-default" id="id72">
<span id="fig-scaling-sample-conv"></span><a class="reference internal image-reference" href="../_images/scaling-sample-conv.png"><img alt="../_images/scaling-sample-conv.png" src="../_images/scaling-sample-conv.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.9.10 </span><span class="caption-text">Transformer language model training runs (figure taken from
<span id="id27"></span>).</span><a class="headerlink" href="#id72" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>As well as increased performance, large models also enjoy better sample
efficiency than small models. <a class="reference internal" href="#fig-scaling-sample-conv"><span class="std std-numref">Fig. 11.9.10</span></a> shows
that large models need fewer training samples (tokens processed) to
perform at the same level achieved by small models, and performance is
scaled smoothly with compute.</p>
<figure class="align-default" id="id73">
<span id="fig-scaling-gpt3"></span><a class="reference internal image-reference" href="../_images/scaling-gpt3.png"><img alt="../_images/scaling-gpt3.png" src="../_images/scaling-gpt3.png" style="width: 250px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.9.11 </span><span class="caption-text">GPT-3 performance (cross-entropy validation loss) follows a power-law
trend with the amount of compute used for training. The power-law
behavior observed in <span id="id28"></span> continues for an
additional two orders of magnitude with only small deviations from
the predicted curve. Embedding parameters are excluded from compute
and parameter counts (caption adapted and figure taken from
<span id="id29"></span>).</span><a class="headerlink" href="#id73" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The empirical scaling behaviors in <span id="id30"></span> have
been tested in subsequent large Transformer models. For example, GPT-3
supported this hypothesis with two more orders of magnitude in
<a class="reference internal" href="#fig-scaling-gpt3"><span class="std std-numref">Fig. 11.9.11</span></a>.</p>
</section>
<section id="large-language-models">
<h2><span class="section-number">11.9.5. </span>Large Language Models<a class="headerlink" href="#large-language-models" title="Permalink to this heading">¶</a></h2>
<p>The scalability of Transformers in the GPT series has inspired
subsequent large language models. The GPT-2 Transformer decoder was used
for training the 530-billion-parameter Megatron-Turing NLG
<span id="id31">()</span> with 270 billion training tokens. Following the
GPT-2 design, the 280-billion-parameter Gopher <span id="id32">()</span>
pretrained with 300 billion tokens, performed competitively across
diverse tasks. Inheriting the same architecture and using the same
compute budget of Gopher, Chinchilla <span id="id33">()</span> is a
substantially smaller (70 billion parameters) model that trains for much
longer (1.4 trillion training tokens), outperforming Gopher on many
tasks and with more emphasis on the number of tokens than on the number
of parameters. To continue the scaling line of language modeling, PaLM
(Pathway Language Model) <span id="id34">()</span>, a
540-billion-parameter Transformer decoder with modified designs
pretrained on 780 billion tokens, outperformed average human performance
on the BIG-Bench benchmark <span id="id35">()</span>. Its later
version, PaLM 2 <span id="id36">()</span>, scaled data and model roughly
1:1 and improved multilingual and reasoning capabilities. Other large
language models, such as Minerva <span id="id37">()</span> that
further trains a generalist (PaLM) and Galactica
<span id="id38">()</span> that is not trained on a general corpus,
have shown promising quantitative and scientific reasoning capabilities.</p>
<p>Open-sourced releases, such as OPT (Open Pretrained Transformers)
<span id="id39">()</span>, BLOOM <span id="id40">()</span>, and FALCON
<span id="id41">()</span>, democratized research and use of large
language models. Focusing on computational efficiency at inference time,
the open-sourced Llama 1 <span id="id42">()</span> outperformed much
larger models by training on more tokens than had been typically used.
The updated Llama 2 <span id="id43">()</span> further increased the
pretraining corpus by 40%, leading to product models that may match the
performance of competitive close-sourced models.</p>
<p><span id="id44"></span> discussed emergent abilities of large
language models that are present in larger models, but not in smaller
models. However, simply increasing model size does not inherently make
models follow human instructions better.
<span id="id45"></span> have found that
fine-tuning large language models on a range of datasets described via
<em>instructions</em> can improve zero-shot performance on held-out tasks.
Using <em>reinforcement learning from human feedback</em>,
<span id="id46"></span> fine-tuned GPT-3 to follow a diverse set
of instructions. Following the resultant InstructGPT which aligns
language models with human intent via fine-tuning
<span id="id47">()</span>, <a class="reference external" href="https://chat.openai.com/">ChatGPT</a> can
generate human-like responses (e.g., code debugging and creative
writing) based on conversations with humans and can perform many natural
language processing tasks zero-shot <span id="id48">()</span>.
<span id="id49"></span> replaced human inputs (e.g.,
human-labeled data) with model outputs to partially automate the
instruction tuning process, which is also known as <em>reinforcement
learning from AI feedback</em>.</p>
<p>Large language models offer an exciting prospect of formulating text
input to induce models to perform desired tasks via in-context learning,
which is also known as <em>prompting</em>. Notably, <em>chain-of-thought
prompting</em> <span id="id50">()</span>, an in-context learning method with
few-shot “question, intermediate reasoning steps, answer”
demonstrations, elicits the complex reasoning capabilities of large
language models in order to solve mathematical, commonsense, and
symbolic reasoning tasks. Sampling multiple reasoning paths
<span id="id51">()</span>, diversifying few-shot demonstrations
<span id="id52">()</span>, and reducing complex problems to
sub-problems <span id="id53">()</span> can all improve the reasoning
accuracy. In fact, with simple prompts like “Let’s think step by step”
just before each answer, large language models can even perform
<em>zero-shot</em> chain-of-thought reasoning with decent accuracy
<span id="id54">()</span>. Even for multimodal inputs consisting of both
text and images, language models can perform multimodal chain-of-thought
reasoning with higher accuracy than using text input only
<span id="id55">()</span>.</p>
</section>
<section id="summary-and-discussion">
<h2><span class="section-number">11.9.6. </span>Summary and Discussion<a class="headerlink" href="#summary-and-discussion" title="Permalink to this heading">¶</a></h2>
<p>Transformers have been pretrained as encoder-only (e.g., BERT),
encoder–decoder (e.g., T5), and decoder-only (e.g., GPT series).
Pretrained models may be adapted to perform different tasks with model
update (e.g., fine-tuning) or not (e.g., few-shot). Scalability of
Transformers suggests that better performance benefits from larger
models, more training data, and more training compute. Since
Transformers were first designed and pretrained for text data, this
section leans slightly towards natural language processing. Nonetheless,
those models discussed above can be often found in more recent models
across multiple modalities. For example, (i) Chinchilla
<span id="id56">()</span> was further extended to Flamingo
<span id="id57">()</span>, a visual language model for few-shot
learning; (ii) GPT-2 <span id="id58">()</span> and the vision
Transformer encode text and images in CLIP (Contrastive Language-Image
Pre-training) <span id="id59">()</span>, whose image and text
embeddings were later adopted in the DALL-E 2 text-to-image system
<span id="id60">()</span>. Although there have been no systematic
studies on Transformer scalability in multimodal pretraining yet, an
all-Transformer text-to-image model called Parti <span id="id61">()</span>
shows potential of scalability across modalities: a larger Parti is more
capable of high-fidelity image generation and content-rich text
understanding (<a class="reference internal" href="#fig-parti"><span class="std std-numref">Fig. 11.9.12</span></a>).</p>
<figure class="align-default" id="id74">
<span id="fig-parti"></span><a class="reference internal image-reference" href="../_images/parti.png"><img alt="../_images/parti.png" src="../_images/parti.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.9.12 </span><span class="caption-text">Image examples generated from the same text by the Parti model of
increasing sizes (350M, 750M, 3B, 20B) (examples taken from
<span id="id62"></span>).</span><a class="headerlink" href="#id74" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="exercises">
<h2><span class="section-number">11.9.7. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>Is it possible to fine-tune T5 using a minibatch consisting of
different tasks? Why or why not? How about for GPT-2?</p></li>
<li><p>Given a powerful language model, what applications can you think of?</p></li>
<li><p>Say that you are asked to fine-tune a language model to perform text
classification by adding additional layers. Where will you add them?
Why?</p></li>
<li><p>Consider sequence-to-sequence problems (e.g., machine translation)
where the input sequence is always available throughout the target
sequence prediction. What could be limitations of modeling with
decoder-only Transformers? Why?</p></li>
</ol>
<p><a class="reference external" href="https://discuss.d2l.ai/t/9232">Discussions</a></p>
</section>
</section>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">11.9. Large-Scale Pretraining with Transformers</a><ul>
<li><a class="reference internal" href="#encoder-only">11.9.1. Encoder-Only</a><ul>
<li><a class="reference internal" href="#pretraining-bert">11.9.1.1. Pretraining BERT</a></li>
<li><a class="reference internal" href="#fine-tuning-bert">11.9.1.2. Fine-Tuning BERT</a></li>
</ul>
</li>
<li><a class="reference internal" href="#encoderdecoder">11.9.2. Encoder–Decoder</a><ul>
<li><a class="reference internal" href="#pretraining-t5">11.9.2.1. Pretraining T5</a></li>
<li><a class="reference internal" href="#fine-tuning-t5">11.9.2.2. Fine-Tuning T5</a></li>
</ul>
</li>
<li><a class="reference internal" href="#decoder-only">11.9.3. Decoder-Only</a><ul>
<li><a class="reference internal" href="#gpt-and-gpt-2">11.9.3.1. GPT and GPT-2</a></li>
<li><a class="reference internal" href="#gpt-3-and-beyond">11.9.3.2. GPT-3 and Beyond</a></li>
</ul>
</li>
<li><a class="reference internal" href="#scalability">11.9.4. Scalability</a></li>
<li><a class="reference internal" href="#large-language-models">11.9.5. Large Language Models</a></li>
<li><a class="reference internal" href="#summary-and-discussion">11.9.6. Summary and Discussion</a></li>
<li><a class="reference internal" href="#exercises">11.9.7. Exercises</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="vision-transformer.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>11.8. Transformers for Vision</div>
         </div>
     </a>
     <a id="button-next" href="../chapter_optimization/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>12. Optimization Algorithms</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>